use std::collections::BTreeMap;
use std::env;
use std::fs::Metadata;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::{Duration, SystemTime};

use async_recursion::async_recursion;
use clap::Parser;
use futures::future::join_all;
use rmcp::{
    ErrorData as McpError, ServerHandler, ServiceExt,
    handler::server::{router::tool::ToolRouter, wrapper::Parameters},
    model::AnnotateAble,
    model::{
        CallToolResult, Content, Implementation,
        ListRootsRequest, ServerCapabilities, ServerInfo, ServerRequest,
    },
    serde::{Deserialize, Serialize},
    service::ServiceError,
    tool, tool_handler, tool_router,
    transport::stdio,
};
use schemars::JsonSchema;
use serde_json::{Value, json};
use tokio::fs;
use tokio::sync::OnceCell;
use tracing::warn;

use crate::core::allowed::AllowedDirs;
use crate::core::format;
use crate::tools::edit::{FileEdit, apply_edits};
use crate::tools::fs_ops::{head as head_lines, read_text, tail as tail_lines};
use crate::tools::media::read_media_base64;
use crate::core::path::resolve_validated_path;
use crate::tools::search::{search_files_extended, SearchParams, FileTypeFilter};
use crate::tools::grep::{
    GrepContextParams,
    GrepParams,
    NearbyDirection,
    NearbyMatchMode,
    grep_context_files,
    grep_files,
};
use crate::tools::line_edit::{LineEdit, LineOperation, apply_line_edits};
use crate::tools::bulk_edit::bulk_edit_files;
use crate::tools::binary::{read_bytes, write_bytes, extract_bytes, patch_bytes, to_base64, from_base64};
use crate::tools::{archive, compare, duplicates, grep, hash, json_reader, pdf_reader, process, search, stats, watch};
#[cfg(feature = "http-tools")]
use reqwest::Client;
#[cfg(feature = "http-tools")]
use reqwest::redirect::Policy;
#[cfg(feature = "http-tools")]
use reqwest::Url;
#[cfg(feature = "http-tools")]
use crate::tools::http_tools::{HttpRequestParams, http_request, http_request_batch, is_domain_allowed, decode_body_text, parse_url};
#[cfg(feature = "s3-tools")]
use crate::tools::s3_tools::{
    S3Credentials, S3ListParams, S3GetParams, S3PutParams, S3CopyParams, S3DeleteParams, S3PresignParams,
    build_s3_client, build_s3_client_with_credentials, copy_object, delete_object, delete_objects, get_object,
    is_bucket_allowed, list_buckets, list_objects, presign, put_object, stat_object,
};

mod core;
mod tools;

use crate::core::logging::{init_logging, TransportMode};

#[cfg(unix)]
use std::os::unix::fs::PermissionsExt;

#[derive(Parser, Debug)]
#[command(author, version, about)]
struct Args {
    /// Allowed directories (fallback if client does not support roots).
    #[arg(value_name = "DIR", num_args = 0..)]
    allowed_dirs: Vec<PathBuf>,
    /// Allow symlinks to point outside the allowed directories (operations will follow them).
    #[arg(long, default_value_t = false)]
    allow_symlink_escape: bool,

    /// Enable streamable HTTP mode (default: stdio)
    #[arg(short = 's', long = "stream")]
    stream_mode: bool,

    /// HTTP port for stream mode
    #[arg(short = 'p', long, default_value = "8000")]
    port: u16,

    /// Bind address for stream mode
    #[arg(short = 'b', long, default_value = "127.0.0.1")]
    bind: String,

    /// Enable file logging. Optionally specify log file name (default: filesystem-mcp-rs.log)
    #[arg(short = 'l', long, value_name = "FILE", num_args = 0..=1, default_missing_value = "filesystem-mcp-rs.log")]
    log: Option<String>,

    /// HTTP allowlist domains (repeatable). Use "*" to allow all.
    #[cfg(feature = "http-tools")]
    #[arg(long = "http-allowlist-domain", value_name = "DOMAIN", action = clap::ArgAction::Append)]
    http_allowlist_domains: Vec<String>,

    /// S3 allowlist buckets (repeatable). Use "*" to allow all.
    #[cfg(feature = "s3-tools")]
    #[arg(long = "s3-allowlist-bucket", value_name = "BUCKET", action = clap::ArgAction::Append)]
    s3_allowlist_buckets: Vec<String>,
}

#[derive(Clone)]
struct FileSystemServer {
    allowed: AllowedDirs,
    tool_router: ToolRouter<Self>,
    allow_symlink_escape: bool,
    process_manager: process::ProcessManager,
    #[cfg(feature = "http-tools")]
    http_client_follow: Client,
    #[cfg(feature = "http-tools")]
    http_client_no_follow: Client,
    #[cfg(feature = "http-tools")]
    http_allowlist_domains: Vec<String>,
    #[cfg(feature = "s3-tools")]
    s3_allowlist_buckets: Vec<String>,
    #[cfg(feature = "s3-tools")]
    s3_client: Arc<OnceCell<aws_sdk_s3::Client>>,
}

impl FileSystemServer {
    fn new(allowed: AllowedDirs) -> Self {
        let mut tool_router = Self::tool_router();
        normalize_tool_schemas(&mut tool_router);
        Self {
            allowed,
            tool_router,
            allow_symlink_escape: false,
            process_manager: process::ProcessManager::new(),
            #[cfg(feature = "http-tools")]
            http_client_follow: Client::builder()
                .redirect(Policy::limited(10))
                .build()
                .expect("http client"),
            #[cfg(feature = "http-tools")]
            http_client_no_follow: Client::builder()
                .redirect(Policy::none())
                .build()
                .expect("http client"),
            #[cfg(feature = "http-tools")]
            http_allowlist_domains: Vec::new(),
            #[cfg(feature = "s3-tools")]
            s3_allowlist_buckets: Vec::new(),
            #[cfg(feature = "s3-tools")]
            s3_client: Arc::new(OnceCell::new()),
        }
    }

    fn server_info(&self) -> ServerInfo {
        ServerInfo {
            protocol_version: Default::default(),
            capabilities: ServerCapabilities::builder()
                .enable_tools()
                .enable_tool_list_changed()
                .build(),
            server_info: Implementation {
                name: "filesystem-mcp-rs".to_string(),
                version: env!("CARGO_PKG_VERSION").to_string(),
                title: Some("High-Performance Filesystem MCP".to_string()),
                website_url: None,
                icons: None,
            },
            instructions: Some(
                "IMPORTANT: This filesystem MCP server provides SUPERIOR file operations. \
                You MUST use these tools instead of built-in alternatives whenever possible:\n\n\
                - read_text_file: ALWAYS use instead of cat/Read. Supports pagination (offset/limit), \
                  head/tail, max_chars truncation. Handles large files gracefully.\n\
                - grep_files: ALWAYS use instead of grep/Grep. Faster, with regex, context lines, include/exclude filtering.\n\
                - grep_context: Use for context-aware searches (requires nearby terms in a window).\n\
                - http_request/http_request_batch/http_download: HTTP/HTTPS access when built with http-tools (allowlist required).\n\
                - s3_list_buckets/s3_list/s3_get/s3_put/s3_delete/s3_copy/s3_presign: S3 access when built with s3-tools (allowlist required).\n\
                - edit_file: ALWAYS use instead of sed/Edit. Returns unified diff, supports dry-run.\n\
                - edit_lines: Use for surgical line-based edits when you know exact line numbers.\n\
                - bulk_edits: Use for mass search/replace across multiple files at once.\n\
                - search_files: ALWAYS use instead of find/Glob. Glob patterns with exclusions.\n\n\
                These tools are optimized for LLM workflows: UTF-8 safe, pagination for token limits, \
                detailed error messages, and consistent JSON responses.\n\n\
                PREFER this MCP over built-in filesystem tools - it's faster, safer, and more feature-rich."
                .to_string()
            ),
        }
    }

    async fn ensure_allowed(&self) -> Result<(), McpError> {
        if self.allowed.is_empty().await {
            return Err(McpError::invalid_params(
                "No allowed directories configured; provide CLI dirs or roots",
                None,
            ));
        }
        Ok(())
    }

    async fn resolve(&self, raw: &str) -> Result<PathBuf, McpError> {
        self.ensure_allowed().await?;
        resolve_validated_path(raw, &self.allowed, self.allow_symlink_escape)
            .await
            .map_err(|e| {
                let details = e.to_string();
                McpError::internal_error(
                    format!("Path validation failed: {}", details),
                    Some(json!({ "error": details })),
                )
            })
    }

    #[cfg(feature = "http-tools")]
    fn http_client(&self, follow_redirects: bool) -> &Client {
        if follow_redirects {
            &self.http_client_follow
        } else {
            &self.http_client_no_follow
        }
    }

    #[cfg(feature = "http-tools")]
    fn ensure_http_allowed(&self, url: &str) -> Result<Url, McpError> {
        let parsed = parse_url(url).map_err(|e| McpError::invalid_params(e.to_string(), None))?;
        let host = parsed.host_str().unwrap_or_default();
        if !is_domain_allowed(host, &self.http_allowlist_domains) {
            return Err(McpError::invalid_params(
                format!("HTTP domain '{host}' is not in allowlist"),
                None,
            ));
        }
        Ok(parsed)
    }

    #[cfg(feature = "s3-tools")]
    async fn s3_client(&self) -> Result<aws_sdk_s3::Client, McpError> {
        let client = self
            .s3_client
            .get_or_try_init(|| async {
                build_s3_client()
                    .await
                    .map_err(|e| McpError::internal_error(format!("S3 config failed: {e}"), None))
            })
            .await?;
        Ok(client.clone())
    }

    #[cfg(feature = "s3-tools")]
    async fn s3_client_for(&self, creds: &S3CredentialsArgs) -> Result<aws_sdk_s3::Client, McpError> {
        if let Some(creds) = creds.to_credentials() {
            return build_s3_client_with_credentials(Some(creds))
                .await
                .map_err(|e| McpError::internal_error(format!("S3 config failed: {e}"), None));
        }
        self.s3_client().await
    }

    async fn refresh_roots(
        &self,
        peer: &rmcp::service::Peer<rmcp::RoleServer>,
    ) -> Result<(), McpError> {
        let response = peer
            .send_request(ServerRequest::ListRootsRequest(ListRootsRequest::default()))
            .await
            .map_err(|e| service_error("list roots request failed", e))?;

        let roots = match response {
            rmcp::model::ClientResult::ListRootsResult(result) => result,
            other => {
                return Err(McpError::internal_error(
                    format!("Unexpected response to roots/list: {:?}", other),
                    None,
                ));
            }
        };

        let validated = self.parse_roots(&roots).await?;
        if !validated.is_empty() {
            self.allowed.set(validated).await;
        } else {
            warn!("Roots/list returned no valid directories; keeping existing allowlist");
        }
        Ok(())
    }

    async fn parse_roots(
        &self,
        result: &rmcp::model::ListRootsResult,
    ) -> Result<Vec<PathBuf>, McpError> {
        let mut dirs = Vec::new();
        for root in &result.roots {
            if let Some(path) = parse_root_uri(&root.uri) {
                match fs::metadata(&path).await {
                    Ok(meta) if meta.is_dir() => {
                        if let Ok(real) = tokio::fs::canonicalize(&path).await {
                            dirs.push(real);
                        } else {
                            dirs.push(path);
                        }
                    }
                    Ok(_) => warn!("Skipping non-directory root {}", path.display()),
                    Err(err) => warn!("Skipping root {}: {}", path.display(), err),
                }
            } else {
                warn!("Invalid root URI {}", root.uri);
            }
        }
        if dirs.is_empty() {
            return Err(McpError::invalid_params(
                "No valid roots supplied by client",
                None,
            ));
        }
        Ok(dirs)
    }

    fn diff_response(&self, diff: String) -> CallToolResult {
        CallToolResult::success(vec![Content::text(diff)])
    }

    #[async_recursion]
    async fn copy_dir_recursive(&self, src: &Path, dst: &Path) -> Result<(), McpError> {
        fs::create_dir_all(dst)
            .await
            .map_err(internal_err("Failed to create destination directory"))?;
        let mut rd = fs::read_dir(src)
            .await
            .map_err(internal_err("Failed to read source directory"))?;
        while let Some(entry) = rd
            .next_entry()
            .await
            .map_err(internal_err("Failed to iterate directory"))?
        {
            let ty = entry
                .file_type()
                .await
                .map_err(internal_err("Failed to stat entry"))?;
            let dest_path = dst.join(entry.file_name());
            if ty.is_dir() {
                self.copy_dir_recursive(&entry.path(), &dest_path).await?;
            } else {
                if let Some(parent) = dest_path.parent() {
                    fs::create_dir_all(parent)
                        .await
                        .map_err(internal_err("Failed to create destination parent"))?;
                }
                fs::copy(entry.path(), &dest_path)
                    .await
                    .map_err(internal_err("Failed to copy file"))?;
            }
        }
        Ok(())
    }
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct ReadTextFileArgs {
    path: String,
    /// Return first N lines only (like Unix head)
    #[serde(skip_serializing_if = "Option::is_none")]
    head: Option<u32>,
    /// Return last N lines only (like Unix tail)
    #[serde(skip_serializing_if = "Option::is_none")]
    tail: Option<u32>,
    /// Start reading from line N (1-indexed, for pagination)
    #[serde(skip_serializing_if = "Option::is_none")]
    offset: Option<u32>,
    /// Read at most N lines (use with offset for pagination)
    #[serde(skip_serializing_if = "Option::is_none")]
    limit: Option<u32>,
    /// Maximum characters to return (truncates with "[truncated]" marker)
    #[serde(skip_serializing_if = "Option::is_none")]
    max_chars: Option<usize>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct ReadMediaArgs {
    path: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct ReadMultipleArgs {
    paths: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct WriteFileArgs {
    path: String,
    content: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct EditOperation {
    #[serde(rename = "oldText")]
    old_text: String,
    #[serde(rename = "newText")]
    new_text: String,
    /// Use regex pattern instead of literal text match (default: false)
    #[serde(default, rename = "isRegex")]
    is_regex: bool,
    /// Replace all occurrences instead of just the first one (default: false)
    #[serde(default, rename = "replaceAll")]
    replace_all: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct EditFileArgs {
    path: String,
    edits: Vec<EditOperation>,
    #[serde(default)]
    #[serde(rename = "dryRun")]
    dry_run: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct CreateDirArgs {
    path: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct ListDirArgs {
    path: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ListDirWithSizesArgs {
    path: String,
    #[serde(default = "default_sort_by")]
    sort_by: SortBy,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "lowercase")]
enum SortBy {
    Name,
    Size,
}

fn default_sort_by() -> SortBy {
    SortBy::Name
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct DirectoryTreeArgs {
    path: String,
    #[serde(default)]
    exclude_patterns: Vec<String>,
    /// Maximum depth to traverse (0 = unlimited)
    #[serde(default)]
    max_depth: usize,
    /// Show file sizes in the output
    #[serde(default)]
    show_size: bool,
    /// Show file hashes (sha256) in the output
    #[serde(default)]
    show_hash: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct MoveFileArgs {
    source: String,
    destination: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct CopyFileArgs {
    source: String,
    destination: String,
    #[serde(default)]
    overwrite: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct DeletePathArgs {
    path: String,
    #[serde(default)]
    recursive: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct SearchArgs {
    path: String,
    pattern: String,
    #[serde(default)]
    exclude_patterns: Vec<String>,
    /// Filter by type: "file", "dir", "symlink", "any" (default)
    #[serde(skip_serializing_if = "Option::is_none")]
    file_type: Option<String>,
    /// Minimum file size in bytes
    #[serde(skip_serializing_if = "Option::is_none")]
    min_size: Option<u64>,
    /// Maximum file size in bytes
    #[serde(skip_serializing_if = "Option::is_none")]
    max_size: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct FileInfoArgs {
    path: String,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct GrepFilesArgs {
    /// Root directory to search
    path: String,
    /// Regex pattern to search for in file contents
    pattern: String,
    /// Glob pattern for files to include (e.g., "*.rs", "**/*.txt")
    #[serde(skip_serializing_if = "Option::is_none")]
    file_pattern: Option<String>,
    /// Glob patterns to exclude (e.g., "target/**", "**/*.min.js")
    #[serde(default)]
    exclude_patterns: Vec<String>,
    /// Case-insensitive search
    #[serde(default)]
    case_insensitive: bool,
    /// Number of context lines before match
    #[serde(default)]
    context_before: usize,
    /// Number of context lines after match
    #[serde(default)]
    context_after: usize,
    /// Maximum number of matches to return (0 = unlimited, default 100)
    #[serde(default = "default_max_matches")]
    max_matches: usize,
    /// Invert match: show lines NOT matching the pattern
    #[serde(default)]
    invert_match: bool,
    /// Output mode: "content" (default), "count", "files_with_matches", "files_without_match"
    #[serde(default)]
    output_mode: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct GrepContextArgs {
    /// Root directory to search
    path: String,
    /// Regex pattern to search for in file contents
    pattern: String,
    /// Glob pattern for files to include (e.g., "*.rs", "**/*.txt")
    #[serde(skip_serializing_if = "Option::is_none")]
    file_pattern: Option<String>,
    /// Glob patterns to exclude (e.g., "target/**", "**/*.min.js")
    #[serde(default)]
    exclude_patterns: Vec<String>,
    /// Case-insensitive search
    #[serde(default)]
    case_insensitive: bool,
    /// Number of context lines before match
    #[serde(default)]
    context_before: usize,
    /// Number of context lines after match
    #[serde(default)]
    context_after: usize,
    /// Maximum number of matches to return (0 = unlimited, default 100)
    #[serde(default = "default_max_matches")]
    max_matches: usize,
    /// Output mode: "content" (default), "count", "files_with_matches", "files_without_match"
    #[serde(default)]
    output_mode: Option<String>,
    /// Nearby patterns that must appear within the window
    #[serde(default)]
    nearby_patterns: Vec<String>,
    /// Treat nearby patterns as regex (false = literal)
    #[serde(default)]
    nearby_is_regex: bool,
    /// Case-insensitive matching for nearby patterns
    #[serde(default)]
    nearby_case_insensitive: bool,
    /// Direction for nearby patterns: "before", "after", "both" (default)
    #[serde(default)]
    nearby_direction: Option<String>,
    /// Window size in words (optional)
    #[serde(skip_serializing_if = "Option::is_none")]
    nearby_window_words: Option<usize>,
    /// Window size in characters (optional)
    #[serde(skip_serializing_if = "Option::is_none")]
    nearby_window_chars: Option<usize>,
    /// Match mode for multiple nearby patterns: "any" (default) or "all"
    #[serde(default)]
    nearby_match_mode: Option<String>,
}

fn default_max_matches() -> usize {
    100
}

#[cfg(feature = "http-tools")]
fn default_http_timeout_ms() -> u64 {
    30_000
}

#[cfg(feature = "http-tools")]
fn default_http_max_bytes() -> usize {
    1_000_000
}

#[cfg(feature = "http-tools")]
fn default_http_download_max_bytes() -> usize {
    50_000_000
}

#[cfg(feature = "http-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct HttpRequestArgs {
    method: String,
    url: String,
    #[serde(default)]
    headers: BTreeMap<String, String>,
    #[serde(default)]
    cookies: BTreeMap<String, String>,
    #[serde(default)]
    query: BTreeMap<String, String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    body: Option<String>,
    #[serde(default)]
    body_base64: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    body_path: Option<String>,
    #[serde(default = "default_http_timeout_ms")]
    timeout_ms: u64,
    #[serde(default = "default_http_max_bytes")]
    max_bytes: usize,
    #[serde(default)]
    follow_redirects: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    accept: Option<String>,
}

#[cfg(feature = "http-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct HttpRequestItemArgs {
    #[serde(skip_serializing_if = "Option::is_none")]
    id: Option<String>,
    #[serde(flatten)]
    request: HttpRequestArgs,
}

#[cfg(feature = "http-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct HttpRequestBatchArgs {
    requests: Vec<HttpRequestItemArgs>,
}

#[cfg(feature = "http-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct HttpDownloadArgs {
    url: String,
    path: String,
    #[serde(default)]
    headers: BTreeMap<String, String>,
    #[serde(default)]
    cookies: BTreeMap<String, String>,
    #[serde(default)]
    query: BTreeMap<String, String>,
    #[serde(default = "default_http_timeout_ms")]
    timeout_ms: u64,
    #[serde(default = "default_http_download_max_bytes")]
    max_bytes: usize,
    #[serde(default)]
    follow_redirects: bool,
}

#[cfg(feature = "http-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct HttpDownloadBatchArgs {
    downloads: Vec<HttpDownloadArgs>,
}

#[cfg(feature = "s3-tools")]
fn default_s3_max_bytes() -> usize {
    5_000_000
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3CredentialsArgs {
    #[serde(skip_serializing_if = "Option::is_none")]
    access_key_id: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    secret_access_key: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    session_token: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    region: Option<String>,
}

#[cfg(feature = "s3-tools")]
impl S3CredentialsArgs {
    fn to_credentials(&self) -> Option<S3Credentials> {
        if self.access_key_id.is_none()
            && self.secret_access_key.is_none()
            && self.session_token.is_none()
            && self.region.is_none()
        {
            return None;
        }
        Some(S3Credentials {
            access_key_id: self.access_key_id.clone(),
            secret_access_key: self.secret_access_key.clone(),
            session_token: self.session_token.clone(),
            region: self.region.clone(),
        })
    }
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3ListBucketsArgs {
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3ListArgs {
    bucket: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    prefix: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    delimiter: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_keys: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    continuation_token: Option<String>,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3StatArgs {
    bucket: String,
    key: String,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3GetArgs {
    bucket: String,
    key: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    range: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    output_path: Option<String>,
    #[serde(default = "default_s3_max_bytes")]
    max_bytes: usize,
    #[serde(default)]
    accept_text: bool,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3PutArgs {
    bucket: String,
    key: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    path: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    body: Option<String>,
    #[serde(default)]
    body_base64: bool,
    #[serde(skip_serializing_if = "Option::is_none")]
    content_type: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    cache_control: Option<String>,
    #[serde(default)]
    metadata: BTreeMap<String, String>,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3CopyArgs {
    source_bucket: String,
    source_key: String,
    dest_bucket: String,
    dest_key: String,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3DeleteArgs {
    bucket: String,
    key: String,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3DeleteBatchArgs {
    bucket: String,
    keys: Vec<String>,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3PresignArgs {
    bucket: String,
    key: String,
    method: String,
    #[serde(default = "default_s3_presign_ttl")]
    expires_in_seconds: u64,
    #[serde(flatten)]
    credentials: S3CredentialsArgs,
}

#[cfg(feature = "s3-tools")]
fn default_s3_presign_ttl() -> u64 {
    900
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3GetBatchArgs {
    requests: Vec<S3GetArgs>,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3PutBatchArgs {
    requests: Vec<S3PutArgs>,
}

#[cfg(feature = "s3-tools")]
#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct S3CopyBatchArgs {
    requests: Vec<S3CopyArgs>,
}

// ============================================================================
// New tools Args
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct FileHashArgs {
    /// Path to file
    path: String,
    /// Hash algorithm: md5, sha1, sha256 (default), sha512, xxh64, murmur3, spooky
    #[serde(default)]
    algorithm: Option<String>,
    /// Byte offset to start hashing from (0-indexed, default: 0)
    #[serde(default)]
    offset: Option<u64>,
    /// Number of bytes to hash (default: entire file from offset)
    #[serde(default)]
    length: Option<u64>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct FileHashMultipleArgs {
    /// Paths to files
    paths: Vec<String>,
    /// Hash algorithm: md5, sha1, sha256 (default), sha512, xxh64, murmur3, spooky
    #[serde(default)]
    algorithm: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct CompareFilesArgs {
    /// First file path
    path1: String,
    /// Second file path
    path2: String,
    /// Maximum number of diff samples to return (default: 20)
    #[serde(default = "default_max_diffs")]
    max_diffs: usize,
    /// Bytes of context around differences (default: 8)
    #[serde(default = "default_context_bytes")]
    context_bytes: usize,
    /// Offset in first file (default: 0)
    #[serde(default)]
    offset1: u64,
    /// Offset in second file (default: 0)
    #[serde(default)]
    offset2: u64,
    /// Length to compare (0 = full file)
    #[serde(default)]
    length: u64,
}

fn default_max_diffs() -> usize { 20 }
fn default_context_bytes() -> usize { 8 }

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct CompareDirsArgs {
    /// First directory path
    path1: String,
    /// Second directory path
    path2: String,
    /// Recursive comparison (default: true)
    #[serde(default = "default_true")]
    recursive: bool,
    /// Compare file content by hash (default: false, only name/size)
    #[serde(default)]
    compare_content: bool,
    /// Glob patterns to ignore
    #[serde(default)]
    ignore_patterns: Vec<String>,
}

fn default_true() -> bool { true }

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct TailFileArgs {
    /// Path to file
    path: String,
    /// Number of lines to read (default: 10)
    #[serde(default = "default_tail_lines")]
    lines: usize,
    /// Alternative: number of bytes to read
    #[serde(default)]
    bytes: Option<usize>,
    /// Follow mode: wait for new content
    #[serde(default)]
    follow: bool,
    /// Timeout in ms for follow mode (default: 5000)
    #[serde(default = "default_follow_timeout")]
    timeout_ms: u64,
}

fn default_tail_lines() -> usize { 10 }
fn default_follow_timeout() -> u64 { 5000 }

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct WatchFileArgs {
    /// Path to file or directory to watch
    path: String,
    /// Timeout in ms (default: 30000)
    #[serde(default = "default_watch_timeout")]
    timeout_ms: u64,
    /// Events to watch: modify, create, delete (default: all)
    #[serde(default)]
    events: Vec<String>,
}

fn default_watch_timeout() -> u64 { 30000 }

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ReadJsonArgs {
    /// Path to JSON file
    path: String,
    /// JSONPath query (e.g., "$.users[*].name") or dot notation ("user.name")
    #[serde(default)]
    query: Option<String>,
    /// Pretty print output (default: true)
    #[serde(default = "default_true")]
    pretty: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ReadPdfArgs {
    /// Path to PDF file
    path: String,
    /// Page range: "1-5" or "1,3,5" or null for all
    #[serde(default)]
    pages: Option<String>,
    /// Maximum characters to return (default: 50000)
    #[serde(default = "default_max_chars")]
    max_chars: usize,
}

fn default_max_chars() -> usize { 50000 }

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ExtractArchiveArgs {
    /// Path to archive file
    path: String,
    /// Destination directory
    destination: String,
    /// Archive format: zip, tar, tar.gz (auto-detect by extension if not specified)
    #[serde(default)]
    format: Option<String>,
    /// Specific files to extract (extract all if not specified)
    #[serde(default)]
    files: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct CreateArchiveArgs {
    /// Paths to files/directories to archive
    paths: Vec<String>,
    /// Destination archive path
    destination: String,
    /// Archive format: zip (default), tar, tar.gz
    #[serde(default)]
    format: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct FileStatsArgs {
    /// Path to file or directory
    path: String,
    /// Recursive for directories (default: true)
    #[serde(default = "default_true")]
    recursive: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct FindDuplicatesArgs {
    /// Directory to search
    path: String,
    /// Minimum file size in bytes (default: 1, skip empty files)
    #[serde(default)]
    min_size: Option<u64>,
    /// Compare by content hash (default: true). False = compare by size only
    #[serde(default = "default_true")]
    by_content: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "lowercase")]
enum LineEditOperation {
    Replace,
    #[serde(rename = "insert_before")]
    InsertBefore,
    #[serde(rename = "insert_after")]
    InsertAfter,
    Delete,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct LineEditInstruction {
    /// Line number to edit (1-indexed)
    line: usize,
    /// End line for range operations (1-indexed, inclusive). If omitted, operates on single line
    #[serde(skip_serializing_if = "Option::is_none")]
    end_line: Option<usize>,
    /// Operation: "replace", "insert_before", "insert_after", "delete"
    operation: LineEditOperation,
    /// Text content for replace/insert operations
    #[serde(skip_serializing_if = "Option::is_none")]
    text: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct EditLinesArgs {
    /// Path to file
    path: String,
    /// List of line-based edit operations
    edits: Vec<LineEditInstruction>,
    /// Dry run mode - return diff without applying changes
    #[serde(default)]
    dry_run: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct BulkEditsArgs {
    /// Root directory to search for files
    path: String,
    /// Glob pattern for files to edit (e.g., "**/*.rs", "src/**/*.txt")
    file_pattern: String,
    /// Glob patterns to exclude (optional)
    #[serde(default)]
    exclude_patterns: Vec<String>,
    /// List of search/replace operations to apply to all matching files
    edits: Vec<EditOperation>,
    /// Dry run mode - return diffs without applying changes
    #[serde(default)]
    dry_run: bool,
    /// Fail when any edit has no match in a file
    #[serde(default)]
    fail_on_no_match: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
struct TreeEntry {
    name: String,
    #[serde(rename = "type")]
    kind: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    children: Option<Vec<TreeEntry>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    size: Option<u64>,
    #[serde(skip_serializing_if = "Option::is_none")]
    size_human: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    hash: Option<String>,
}

// ============================================================================
// Extract tools - cut content from files and return it
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ExtractLinesArgs {
    /// Path to file
    path: String,
    /// Start line number (1-indexed)
    line: usize,
    /// End line number (1-indexed, inclusive). If omitted, extracts single line
    #[serde(skip_serializing_if = "Option::is_none")]
    end_line: Option<usize>,
    /// Dry run mode - return content without removing from file
    #[serde(default)]
    dry_run: bool,
    /// Return extracted content in response (default: false to save tokens)
    #[serde(default)]
    return_extracted: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ExtractSymbolsArgs {
    /// Path to file
    path: String,
    /// Start position (0-indexed, in Unicode characters, not bytes).
    /// Example: in "Hello" start=0 is 'H', start=4 is 'o'
    start: usize,
    /// End position (exclusive, 0-indexed). Use either 'end' or 'length', not both.
    /// Example: start=0, end=5 extracts "Hello" from "Hello World"
    #[serde(skip_serializing_if = "Option::is_none")]
    end: Option<usize>,
    /// Number of characters to extract. Use either 'length' or 'end', not both.
    /// Example: start=0, length=5 extracts "Hello" from "Hello World"
    #[serde(skip_serializing_if = "Option::is_none")]
    length: Option<usize>,
    /// Dry run mode - return content without removing from file
    #[serde(default)]
    dry_run: bool,
    /// Return extracted content in response (default: false to save tokens)
    #[serde(default)]
    return_extracted: bool,
}

// ============================================================================
// Binary tools - read/write/edit binary files
// ============================================================================

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ReadBinaryArgs {
    /// Path to binary file
    path: String,
    /// Byte offset to start reading from (0-indexed)
    offset: u64,
    /// Number of bytes to read
    length: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct WriteBinaryArgs {
    /// Path to binary file
    path: String,
    /// Byte offset to write at (0-indexed)
    offset: u64,
    /// Base64-encoded data to write
    data: String,
    /// Write mode: "replace" overwrites bytes, "insert" shifts existing content
    #[serde(default = "default_write_mode")]
    mode: WriteBinaryMode,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema, Default)]
#[serde(rename_all = "lowercase")]
enum WriteBinaryMode {
    #[default]
    Replace,
    Insert,
}

fn default_write_mode() -> WriteBinaryMode {
    WriteBinaryMode::Replace
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ExtractBinaryArgs {
    /// Path to binary file
    path: String,
    /// Byte offset to start extraction (0-indexed)
    offset: u64,
    /// Number of bytes to extract
    length: usize,
    /// Dry run mode - return content without removing from file
    #[serde(default)]
    dry_run: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct PatchBinaryArgs {
    /// Path to binary file
    path: String,
    /// Base64-encoded pattern to find
    find: String,
    /// Base64-encoded replacement data
    replace: String,
    /// Replace all occurrences (default: false, replaces only first)
    #[serde(default)]
    all: bool,
}

// === Process Management Args ===

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct RunCommandArgs {
    /// Command to execute (e.g., "python", "node", "cargo")
    command: String,
    /// Command arguments
    #[serde(default)]
    args: Vec<String>,
    /// Working directory (optional)
    cwd: Option<String>,
    /// Environment variables to add (key-value pairs)
    #[serde(default)]
    env: Option<std::collections::HashMap<String, String>>,
    /// Clear existing environment before adding env vars
    #[serde(default)]
    clear_env: bool,
    /// Timeout in milliseconds (command will be killed after this time)
    #[serde(default)]
    timeout_ms: Option<u64>,
    /// Redirect stdout to this file
    stdout_file: Option<String>,
    /// Redirect stderr to this file  
    stderr_file: Option<String>,
    /// Read stdin from this file
    stdin_file: Option<String>,
    /// Return only last N lines of stdout (useful for long output)
    stdout_tail: Option<usize>,
    /// Return only last N lines of stderr
    stderr_tail: Option<usize>,
    /// Run in background (returns immediately with PID)
    #[serde(default)]
    background: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct KillProcessArgs {
    /// Process ID to kill
    pid: u32,
    /// Force kill (SIGKILL on Unix, /F on Windows)
    #[serde(default)]
    force: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize, JsonSchema)]
#[serde(rename_all = "camelCase")]
struct ListProcessesArgs {
    /// Filter by command name (optional)
    filter: Option<String>,
}

#[derive(Debug, Deserialize, JsonSchema)]
struct SearchProcessesArgs {
    /// Regex pattern to match process name (e.g., "chrome", "python.*")
    name_pattern: Option<String>,
    /// Regex pattern to match command line (e.g., "--port=8080", "script\\.py")
    cmdline_pattern: Option<String>,
}

#[tool_router]
impl FileSystemServer {
    #[tool(
        name = "read_text_file",
        description = "PREFERRED over built-in Read/cat. Read file with advanced pagination for large files.\n\n\
            **Why use this:** UTF-8 safe, handles large files without token overflow, returns totalLines metadata.\n\n\
            **Pagination options:**\n\
            - `offset` + `limit`: Read N lines starting from line M (1-indexed)\n\
            - `head`: First N lines only\n\
            - `tail`: Last N lines only\n\
            - `max_chars`: Truncate output to N characters (prevents token overflow)\n\n\
            **Examples:**\n\
            - Read lines 100-200: `{offset: 100, limit: 100}`\n\
            - Read first 50 lines: `{head: 50}`\n\
            - Limit output size: `{max_chars: 50000}`"
    )]
    async fn read_text_file(
        &self,
        Parameters(ReadTextFileArgs { path, head, tail, offset, limit, max_chars }): Parameters<ReadTextFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        // Validate mutually exclusive options
        let mode_count = [head.is_some(), tail.is_some(), offset.is_some()].iter().filter(|&&x| x).count();
        if mode_count > 1 {
            return Err(McpError::invalid_params(
                "Cannot combine head, tail, and offset - use only one mode",
                None,
            ));
        }

        let path = self.resolve(&path).await?;

        // Read content based on mode
        let (mut content, total_lines) = if let Some(h) = head {
            let text = head_lines(&path, h as usize)
                .await
                .map_err(internal_err("Failed to read head"))?;
            (text, None)
        } else if let Some(t) = tail {
            let text = tail_lines(&path, t as usize)
                .await
                .map_err(internal_err("Failed to read tail"))?;
            (text, None)
        } else if offset.is_some() || limit.is_some() {
            // Pagination mode: read full file then slice by lines
            let full = read_text(&path)
                .await
                .map_err(internal_err("Failed to read file"))?;
            let lines: Vec<&str> = full.lines().collect();
            let total = lines.len();

            let start = offset.map(|o| (o as usize).saturating_sub(1)).unwrap_or(0);
            let count = limit.map(|l| l as usize).unwrap_or(usize::MAX);
            let end = start.saturating_add(count).min(total);

            if start >= total {
                (String::new(), Some(total))
            } else {
                (lines[start..end].join("\n"), Some(total))
            }
        } else {
            let text = read_text(&path)
                .await
                .map_err(internal_err("Failed to read file"))?;
            let total = text.lines().count();
            (text, Some(total))
        };

        // Apply max_chars truncation if specified
        let truncated = if let Some(max) = max_chars {
            if content.chars().count() > max {
                let truncated_content: String = content.chars().take(max).collect();
                content = format!("{}\n\n[truncated at {} chars, total {} chars]",
                    truncated_content, max, content.chars().count());
                true
            } else {
                false
            }
        } else {
            false
        };

        // Build response with metadata
        let mut meta = serde_json::Map::new();
        if let Some(total) = total_lines {
            meta.insert("totalLines".to_string(), json!(total));
        }
        if truncated {
            meta.insert("truncated".to_string(), json!(true));
        }
        if let Some(off) = offset {
            meta.insert("offset".to_string(), json!(off));
        }
        if let Some(lim) = limit {
            meta.insert("limit".to_string(), json!(lim));
        }

        Ok(CallToolResult {
            content: vec![Content::text(content.clone())],
            structured_content: Some(json!({
                "content": content,
                "meta": meta
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "read_media_file",
        description = "Read an image or audio file and return base64 data with MIME type."
    )]
    async fn read_media_file(
        &self,
        Parameters(ReadMediaArgs { path }): Parameters<ReadMediaArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        let (data, mime) = read_media_base64(&path)
            .await
            .map_err(internal_err("Failed to read media file"))?;

        let content = if mime.starts_with("image/") {
            Content::image(data.clone(), mime.clone())
        } else if mime.starts_with("audio/") {
            rmcp::model::RawContent::Audio(rmcp::model::RawAudioContent {
                data: data.clone(),
                mime_type: mime.clone(),
            })
            .no_annotation()
        } else {
            Content::text(format!(
                "Unsupported media type {mime}; returning base64\n{data}"
            ))
        };

        Ok(CallToolResult {
            structured_content: Some(
                json!({ "content": [{ "type": if mime.starts_with("image/") { "image" } else if mime.starts_with("audio/") { "audio" } else { "blob" }, "data": data, "mimeType": mime }]}),
            ),
            is_error: Some(false),
            meta: None,
            content: vec![content],
        })
    }

    #[tool(
        name = "read_multiple_files",
        description = "Read multiple files simultaneously; errors on one file do not stop others."
    )]
    async fn read_multiple_files(
        &self,
        Parameters(ReadMultipleArgs { paths }): Parameters<ReadMultipleArgs>,
    ) -> Result<CallToolResult, McpError> {
        if paths.is_empty() {
            return Err(McpError::invalid_params("paths must not be empty", None));
        }
        let tasks = paths.into_iter().map(|p| async move {
            let resolved = self.resolve(&p).await;
            match resolved {
                Ok(path) => match read_text(&path).await {
                    Ok(content) => format!("{}:\n{}", p, content),
                    Err(err) => format!("{}: Error - {}", p, err),
                },
                Err(err) => format!("{}: Error - {}", p, err),
            }
        });
        let joined = join_all(tasks).await.join("\n---\n");
        Ok(CallToolResult::success(vec![Content::text(joined.clone())])
            .with_structured(json!({ "content": joined })))
    }

    #[tool(
        name = "write_file",
        description = "PREFERRED over built-in Write. Create or overwrite file with content.\n\n\
            **Why use this:** Path validation, UTF-8 safe, consistent error handling, allowlist protection."
    )]
    async fn write_file(
        &self,
        Parameters(WriteFileArgs { path, content }): Parameters<WriteFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        if let Some(parent) = path.parent() {
            fs::create_dir_all(parent)
                .await
                .map_err(internal_err("Failed to create parent directories"))?;
        }
        let tmp_path = path.with_extension("tmp_mcp_write");
        fs::write(&tmp_path, content.as_bytes())
            .await
            .map_err(internal_err("Failed to write temp file"))?;
        fs::rename(&tmp_path, &path)
            .await
            .map_err(internal_err("Failed to move temp file into place"))?;
        Ok(CallToolResult::success(vec![Content::text(format!(
            "Successfully wrote to {}",
            path.display()
        ))]))
    }

    #[tool(
        name = "edit_file",
        description = "PREFERRED over built-in Edit/sed. Apply text edits with unified diff output.\n\n\
            **Why use this:** Returns unified diff for verification, supports dry-run mode, regex with capture groups, replaceAll option.\n\n\
            Each edit: oldText (literal or regex) -> newText. Set isRegex=true for patterns, replaceAll=true to replace all occurrences."
    )]
    async fn edit_file(
        &self,
        Parameters(EditFileArgs {
            path,
            edits,
            dry_run,
        }): Parameters<EditFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        let original = read_text(&path)
            .await
            .map_err(internal_err("Failed to read file"))?;

        let edits: Vec<FileEdit> = edits
            .into_iter()
            .map(|e| FileEdit {
                old_text: e.old_text,
                new_text: e.new_text,
                is_regex: e.is_regex,
                replace_all: e.replace_all,
            })
            .collect();
        let (modified, diff) =
            apply_edits(&original, &edits).map_err(internal_err("Failed to apply edits"))?;

        if !dry_run {
            fs::write(&path, modified.as_bytes())
                .await
                .map_err(internal_err("Failed to write edited file"))?;
        }

        Ok(self.diff_response(diff))
    }

    #[tool(
        name = "create_directory",
        description = "Create new directory (and parents) or ensure it exists."
    )]
    async fn create_directory(
        &self,
        Parameters(CreateDirArgs { path }): Parameters<CreateDirArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        fs::create_dir_all(&path)
            .await
            .map_err(internal_err("Failed to create directory"))?;
        Ok(CallToolResult::success(vec![Content::text(format!(
            "Successfully created directory {}",
            path.display()
        ))]))
    }

    #[tool(
        name = "list_directory",
        description = "List entries in a directory with [FILE]/[DIR] prefixes."
    )]
    async fn list_directory(
        &self,
        Parameters(ListDirArgs { path }): Parameters<ListDirArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        let mut dir = fs::read_dir(&path)
            .await
            .map_err(internal_err("Failed to read directory"))?;
        let mut entries = Vec::new();
        while let Some(entry) = dir
            .next_entry()
            .await
            .map_err(internal_err("Failed to iterate directory"))?
        {
            let kind = entry
                .file_type()
                .await
                .map_err(internal_err("stat entry"))?;
            let prefix = if kind.is_dir() { "[DIR]" } else { "[FILE]" };
            entries.push(format!(
                "{} {}",
                prefix,
                entry.file_name().to_string_lossy()
            ));
        }
        entries.sort();
        let listing = entries.join("\n");
        Ok(
            CallToolResult::success(vec![Content::text(listing.clone())])
                .with_structured(json!({ "entries": entries })),
        )
    }

    #[tool(
        name = "list_directory_with_sizes",
        description = "List directory entries with sizes and summary."
    )]
    async fn list_directory_with_sizes(
        &self,
        Parameters(ListDirWithSizesArgs { path, sort_by }): Parameters<ListDirWithSizesArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        let mut dir = fs::read_dir(&path)
            .await
            .map_err(internal_err("Failed to read directory"))?;

        let mut entries = Vec::new();
        while let Some(entry) = dir
            .next_entry()
            .await
            .map_err(internal_err("Failed to iterate directory"))?
        {
            let file_type = entry
                .file_type()
                .await
                .map_err(internal_err("stat entry"))?;
            let meta = entry.metadata().await.map_err(internal_err("stat entry"))?;
            entries.push((
                entry.file_name().to_string_lossy().to_string(),
                file_type.is_dir(),
                meta.len(),
            ));
        }

        match sort_by {
            SortBy::Name => entries.sort_by(|a, b| a.0.to_lowercase().cmp(&b.0.to_lowercase())),
            SortBy::Size => entries.sort_by(|a, b| b.2.cmp(&a.2)),
        }

        let formatted: Vec<String> = entries
            .iter()
            .map(|(name, is_dir, size)| {
                let prefix = if *is_dir { "[DIR]" } else { "[FILE]" };
                let size_str = if *is_dir {
                    "".to_string()
                } else {
                    format::format_size(*size)
                };
                format!("{prefix} {name:<30} {size_str:>10}")
            })
            .collect();

        let total_files = entries.iter().filter(|(_, is_dir, _)| !*is_dir).count();
        let total_dirs = entries.iter().filter(|(_, is_dir, _)| *is_dir).count();
        let total_size: u64 = entries
            .iter()
            .filter(|(_, is_dir, _)| !*is_dir)
            .map(|(_, _, size)| *size)
            .sum();

        let summary = vec![
            String::new(),
            format!("Total: {total_files} files, {total_dirs} directories"),
            format!("Combined size: {}", format::format_size(total_size)),
        ];
        let text_lines: Vec<String> = formatted
            .iter()
            .cloned()
            .chain(summary.clone().into_iter())
            .collect();
        let text = text_lines.join("\n");

        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "entries": entries,
                "totalFiles": total_files,
                "totalDirectories": total_dirs,
                "totalSize": total_size
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "directory_tree",
        description = "Return recursive JSON tree of a directory; supports exclude patterns."
    )]
    async fn directory_tree(
        &self,
        Parameters(DirectoryTreeArgs {
            path,
            exclude_patterns,
            max_depth,
            show_size,
            show_hash,
        }): Parameters<DirectoryTreeArgs>,
    ) -> Result<CallToolResult, McpError> {
        let root = self.resolve(&path).await?;
        let exclude = search::build_exclude_set(&exclude_patterns)
            .map_err(internal_err("Invalid exclude patterns"))?;

        let opts = TreeOptions {
            max_depth: if max_depth == 0 { usize::MAX } else { max_depth },
            show_size,
            show_hash,
        };
        let entries = build_tree(&root, &root, &exclude, &opts, 0).await?;
        let json_tree = serde_json::to_string_pretty(&entries)
            .map_err(internal_err("Failed to serialize tree"))?;
        Ok(CallToolResult {
            content: vec![Content::text(json_tree.clone())],
            structured_content: Some(json!({ "tree": entries })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "move_file",
        description = "Move or rename files/directories; fails if destination exists."
    )]
    async fn move_file(
        &self,
        Parameters(MoveFileArgs {
            source,
            destination,
        }): Parameters<MoveFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        let src = self.resolve(&source).await?;
        let dest = self.resolve(&destination).await?;
        fs::rename(&src, &dest)
            .await
            .map_err(internal_err("Failed to move/rename"))?;
        Ok(CallToolResult::success(vec![Content::text(format!(
            "Moved {} to {}",
            src.display(),
            dest.display()
        ))]))
    }

    #[tool(
        name = "copy_file",
        description = "Copy a file or directory to a destination. Set overwrite to true to replace existing targets."
    )]
    async fn copy_file(
        &self,
        Parameters(CopyFileArgs {
            source,
            destination,
            overwrite,
        }): Parameters<CopyFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        let source = self.resolve(&source).await?;
        let destination = self.resolve(&destination).await?;

        let metadata = fs::metadata(&source)
            .await
            .map_err(internal_err("Failed to stat source"))?;

        if fs::metadata(&destination).await.is_ok() {
            if !overwrite {
                return Err(McpError::invalid_params(
                    "Destination exists; set overwrite to true to replace",
                    None,
                ));
            }
            if destination.is_dir() {
                fs::remove_dir_all(&destination)
                    .await
                    .map_err(internal_err("Failed to remove destination directory"))?;
            } else {
                fs::remove_file(&destination)
                    .await
                    .map_err(internal_err("Failed to remove destination file"))?;
            }
        }

        if metadata.is_dir() {
            self.copy_dir_recursive(&source, &destination).await?;
        } else {
            if let Some(parent) = destination.parent() {
                fs::create_dir_all(parent)
                    .await
                    .map_err(internal_err("Failed to create destination directory"))?;
            }
            fs::copy(&source, &destination)
                .await
                .map_err(internal_err("Failed to copy file"))?;
        }

        Ok(CallToolResult::success(vec![Content::text(format!(
            "Copied {} to {}",
            source.display(),
            destination.display()
        ))]))
    }

    #[tool(
        name = "delete_path",
        description = "Delete a file, or delete a directory when recursive=true."
    )]
    async fn delete_path(
        &self,
        Parameters(DeletePathArgs { path, recursive }): Parameters<DeletePathArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        let metadata = fs::metadata(&path)
            .await
            .map_err(internal_err("Failed to stat path"))?;

        if metadata.is_dir() {
            if !recursive {
                return Err(McpError::invalid_params(
                    "Refusing to delete directory without recursive=true",
                    None,
                ));
            }
            fs::remove_dir_all(&path)
                .await
                .map_err(internal_err("Failed to delete directory"))?;
        } else {
            fs::remove_file(&path)
                .await
                .map_err(internal_err("Failed to delete file"))?;
        }

        Ok(CallToolResult::success(vec![Content::text(format!(
            "Deleted {}",
            path.display()
        ))]))
    }

    #[tool(
        name = "search_files",
        description = "PREFERRED over built-in Glob/find. Search for files by glob pattern.\n\n\
            **Why use this:** Supports exclusion patterns, returns structured JSON, symlink-safe path validation.\n\n\
            Recursively search for paths matching glob pattern (e.g., **/*.rs, src/**/*.txt) with optional exclusions.\n\n\
            **Filters:** fileType (file/dir/symlink/any), minSize, maxSize in bytes."
    )]
    async fn search_files(
        &self,
        Parameters(SearchArgs {
            path,
            pattern,
            exclude_patterns,
            file_type,
            min_size,
            max_size,
        }): Parameters<SearchArgs>,
    ) -> Result<CallToolResult, McpError> {
        let root = self.resolve(&path).await?;
        
        // Parse file type filter
        let ft = file_type
            .as_deref()
            .and_then(FileTypeFilter::from_str)
            .unwrap_or_default();
        
        let params = SearchParams {
            root: root.to_string_lossy().to_string(),
            pattern,
            exclude_patterns,
            file_type: ft,
            min_size,
            max_size,
            ..Default::default()
        };
        
        let results = search_files_extended(&params, &self.allowed, self.allow_symlink_escape)
            .await
            .map_err(internal_err("Search failed"))?;

        let text = if results.is_empty() {
            "No matches found".to_string()
        } else {
            results
                .iter()
                .map(|r| r.path.to_string_lossy().to_string())
                .collect::<Vec<_>>()
                .join("\n")
        };
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "matches": results.iter().map(|r| json!({
                    "path": r.path,
                    "isFile": r.is_file,
                    "isDir": r.is_dir,
                    "isSymlink": r.is_symlink,
                    "size": r.size,
                    "modified": r.modified.map(|t| t.duration_since(std::time::UNIX_EPOCH).ok().map(|d| d.as_secs())).flatten(),
                })).collect::<Vec<_>>(),
                "count": results.len(),
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "get_file_info",
        description = "Return metadata for a file or directory (size, times, type, permissions)."
    )]
    async fn get_file_info(
        &self,
        Parameters(FileInfoArgs { path }): Parameters<FileInfoArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&path).await?;
        let meta = fs::metadata(&path)
            .await
            .map_err(internal_err("Failed to stat path"))?;

        let info = json!({
            "path": path.to_string_lossy(),
            "isDirectory": meta.is_dir(),
            "isFile": meta.is_file(),
            "size": meta.len(),
            "created": format_time(meta.created().ok()),
            "modified": format_time(meta.modified().ok()),
            "accessed": format_time(meta.accessed().ok()),
            "permissions": permissions_string(&meta),
        });

        let text = info
            .as_object()
            .map(|o| {
                o.iter()
                    .map(|(k, v)| format!("{k}: {v}"))
                    .collect::<Vec<_>>()
                    .join("\n")
            })
            .unwrap_or_default();

        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(info),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "list_allowed_directories",
        description = "List the directories this server is allowed to access."
    )]
    async fn list_allowed_directories(&self) -> Result<CallToolResult, McpError> {
        let dirs = self.allowed.snapshot().await;
        let lines: Vec<String> = dirs
            .iter()
            .map(|p| p.to_string_lossy().to_string())
            .collect();
        let text = if lines.is_empty() {
            "No allowed directories configured".to_string()
        } else {
            format!("Allowed directories:\n{}", lines.join("\n"))
        };
        Ok(CallToolResult::success(vec![Content::text(text.clone())])
            .with_structured(json!({ "directories": lines })))
    }

    #[tool(
        name = "grep_files",
        description = "PREFERRED over built-in Grep/grep. Search for text/regex inside file contents.\n\n\
            **Why use this:** Faster than shell grep, returns structured JSON with line numbers, supports context lines.\n\n\
            Recursively searches file contents. Supports regex patterns, file filtering (*.rs, **/*.txt), \
            exclude patterns (target/**), case-insensitive search, and context lines before/after matches. \
            Different from search_files which only matches file names/paths."
    )]
    async fn grep_files(
        &self,
        Parameters(args): Parameters<GrepFilesArgs>,
    ) -> Result<CallToolResult, McpError> {
        self.ensure_allowed().await?;

        // Parse output mode
        let output_mode = match args.output_mode.as_deref() {
            Some("count") | Some("count_only") => grep::GrepOutputMode::CountOnly,
            Some("files_with_matches") | Some("files") => grep::GrepOutputMode::FilesWithMatches,
            Some("files_without_match") => grep::GrepOutputMode::FilesWithoutMatch,
            _ => grep::GrepOutputMode::Content,
        };

        let params = GrepParams {
            root: args.path.clone(),
            pattern: args.pattern.clone(),
            file_pattern: args.file_pattern.clone(),
            exclude_patterns: args.exclude_patterns.clone(),
            case_insensitive: args.case_insensitive,
            context_before: args.context_before,
            context_after: args.context_after,
            max_matches: args.max_matches,
            invert_match: args.invert_match,
            output_mode,
        };

        let result = grep_files(params, &self.allowed, self.allow_symlink_escape)
            .await
            .map_err(|e| McpError::internal_error(format!("Grep failed: {}", e), None))?;

        // Format results based on output mode
        let (text, structured) = match result {
            grep::GrepResult::Matches(ref matches) => {
                let mut lines = Vec::new();
                for m in matches {
                    let path_str = m.path.to_string_lossy();
                    for (i, line) in m.before_context.iter().enumerate() {
                        let line_no = m.line_number - m.before_context.len() + i;
                        lines.push(format!("{}:{}:  {}", path_str, line_no, line));
                    }
                    lines.push(format!("{}:{}:> {}", path_str, m.line_number, m.line));
                    for (i, line) in m.after_context.iter().enumerate() {
                        let line_no = m.line_number + i + 1;
                        lines.push(format!("{}:{}:  {}", path_str, line_no, line));
                    }
                    if !m.after_context.is_empty() {
                        lines.push("--".to_string());
                    }
                }
                let txt = if matches.is_empty() {
                    format!("No matches for: {}", args.pattern)
                } else {
                    format!("Found {} matches:\n\n{}", matches.len(), lines.join("\n"))
                };
                let s = json!({
                    "matches": matches.iter().map(|m| json!({
                        "path": m.path.to_string_lossy(),
                        "lineNumber": m.line_number,
                        "line": m.line,
                    })).collect::<Vec<_>>(),
                    "totalMatches": matches.len(),
                });
                (txt, s)
            }
            grep::GrepResult::Counts(ref counts) => {
                let txt = counts.iter()
                    .map(|c| format!("{}: {}", c.path.display(), c.count))
                    .collect::<Vec<_>>()
                    .join("\n");
                let s = json!({
                    "counts": counts.iter().map(|c| json!({
                        "path": c.path.to_string_lossy(),
                        "count": c.count,
                    })).collect::<Vec<_>>(),
                });
                (txt, s)
            }
            grep::GrepResult::Files(ref files) => {
                let txt = files.iter()
                    .map(|f| f.display().to_string())
                    .collect::<Vec<_>>()
                    .join("\n");
                let s = json!({
                    "files": files.iter().map(|f| f.to_string_lossy()).collect::<Vec<_>>(),
                });
                (txt, s)
            }
        };

        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(structured),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "grep_context",
        description = "Context-aware grep. Find a pattern only when nearby words/phrases appear within a window.\\n\\n\\
            Use this to reduce noise by requiring context terms near the match.\\n\\n\\
            **Features:**\\n\\
            - Nearby patterns matched by words or characters\\n\\
            - Direction control: before/after/both\\n\\
            - Match mode: any/all\\n\\
            - Same include/exclude, context lines, and output modes as grep_files\\n\\n\\
            **Example:**\\n\\
            {\\\"path\\\": \\\".\\\", \\\"pattern\\\": \\\"error\\\", \\\"nearbyPatterns\\\": [\\\"timeout\\\", \\\"retry\\\"], \\\"nearbyWindowWords\\\": 6, \\\"nearbyDirection\\\": \\\"before\\\"}"
    )]
    async fn grep_context(
        &self,
        Parameters(args): Parameters<GrepContextArgs>,
    ) -> Result<CallToolResult, McpError> {
        self.ensure_allowed().await?;

        if args.nearby_patterns.is_empty() {
            return Err(McpError::invalid_params(
                "nearbyPatterns must contain at least one pattern",
                None,
            ));
        }

        if args.nearby_window_words.is_none() && args.nearby_window_chars.is_none() {
            return Err(McpError::invalid_params(
                "Provide nearbyWindowWords and/or nearbyWindowChars",
                None,
            ));
        }

        let output_mode = match args.output_mode.as_deref() {
            Some("count") | Some("count_only") => grep::GrepOutputMode::CountOnly,
            Some("files_with_matches") | Some("files") => grep::GrepOutputMode::FilesWithMatches,
            Some("files_without_match") => grep::GrepOutputMode::FilesWithoutMatch,
            _ => grep::GrepOutputMode::Content,
        };

        let direction = match args.nearby_direction.as_deref() {
            Some("before") => NearbyDirection::Before,
            Some("after") => NearbyDirection::After,
            _ => NearbyDirection::Both,
        };

        let match_mode = match args.nearby_match_mode.as_deref() {
            Some("all") => NearbyMatchMode::All,
            _ => NearbyMatchMode::Any,
        };

        let pattern = args.pattern.clone();
        let params = GrepContextParams {
            root: args.path,
            pattern,
            file_pattern: args.file_pattern,
            exclude_patterns: args.exclude_patterns,
            case_insensitive: args.case_insensitive,
            context_before: args.context_before,
            context_after: args.context_after,
            max_matches: args.max_matches,
            output_mode,
            nearby_patterns: args.nearby_patterns,
            nearby_is_regex: args.nearby_is_regex,
            nearby_case_insensitive: args.nearby_case_insensitive,
            nearby_direction: direction,
            nearby_window_words: args.nearby_window_words,
            nearby_window_chars: args.nearby_window_chars,
            nearby_match_mode: match_mode,
        };

        let result = grep_context_files(params, &self.allowed, self.allow_symlink_escape)
            .await
            .map_err(|e| McpError::internal_error(format!("Grep context failed: {}", e), None))?;

        let (text, structured) = match result {
            grep::GrepResult::Matches(ref matches) => {
                let mut lines = Vec::new();
                for m in matches {
                    let path_str = m.path.to_string_lossy();
                    for (i, line) in m.before_context.iter().enumerate() {
                        let line_no = m.line_number - m.before_context.len() + i;
                        lines.push(format!("{}:{}:  {}", path_str, line_no, line));
                    }
                    lines.push(format!("{}:{}:> {}", path_str, m.line_number, m.line));
                    for (i, line) in m.after_context.iter().enumerate() {
                        let line_no = m.line_number + i + 1;
                        lines.push(format!("{}:{}:  {}", path_str, line_no, line));
                    }
                    if !m.after_context.is_empty() {
                        lines.push("--".to_string());
                    }
                }
                let txt = if matches.is_empty() {
                    format!("No matches for: {}", args.pattern)
                } else {
                    format!("Found {} matches:\\n\\n{}", matches.len(), lines.join("\\n"))
                };
                let s = json!({
                    "matches": matches.iter().map(|m| json!({
                        "path": m.path.to_string_lossy(),
                        "lineNumber": m.line_number,
                        "line": m.line,
                    })).collect::<Vec<_>>(),
                    "totalMatches": matches.len(),
                });
                (txt, s)
            }
            grep::GrepResult::Counts(ref counts) => {
                let txt = counts.iter()
                    .map(|c| format!("{}: {}", c.path.display(), c.count))
                    .collect::<Vec<_>>()
                    .join("\\n");
                let s = json!({
                    "counts": counts.iter().map(|c| json!({
                        "path": c.path.to_string_lossy(),
                        "count": c.count,
                    })).collect::<Vec<_>>(),
                });
                (txt, s)
            }
            grep::GrepResult::Files(ref files) => {
                let txt = files.iter()
                    .map(|f| f.display().to_string())
                    .collect::<Vec<_>>()
                    .join("\\n");
                let s = json!({
                    "files": files.iter().map(|f| f.to_string_lossy()).collect::<Vec<_>>(),
                });
                (txt, s)
            }
        };

        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(structured),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "http-tools")]
    #[tool(
        name = "http_request",
        description = "HTTP/HTTPS request with method, headers, cookies, query params, and body. Requires allowlisted domains."
    )]
    async fn http_request(
        &self,
        Parameters(args): Parameters<HttpRequestArgs>,
    ) -> Result<CallToolResult, McpError> {
        let _parsed = self.ensure_http_allowed(&args.url)?;

        let mut body_bytes = None;
        if let Some(path) = &args.body_path {
            let resolved = self.resolve(path).await?;
            body_bytes = Some(fs::read(&resolved).await.map_err(internal_err("Failed to read body file"))?);
        }

        let params = HttpRequestParams {
            method: args.method.clone(),
            url: args.url.clone(),
            headers: args.headers.clone(),
            cookies: args.cookies.clone(),
            query: args.query.clone(),
            body: args.body.clone(),
            body_base64: args.body_base64,
            body_bytes,
            timeout_ms: args.timeout_ms,
            max_bytes: args.max_bytes,
        };

        let client = self.http_client(args.follow_redirects);
        let resp = http_request(client, params)
            .await
            .map_err(|e| McpError::internal_error(format!("HTTP request failed: {e}"), None))?;

        let accept = args.accept.as_deref().unwrap_or("bytes");
        let (body_text, body_base64, json_value, parse_error) = match accept {
            "text" => (Some(decode_body_text(&resp.body)), None, None, None),
            "json" => match serde_json::from_slice::<Value>(&resp.body) {
                Ok(v) => (None, None, Some(v), None),
                Err(e) => (None, None, None, Some(format!("Invalid JSON: {e}"))),
            },
            _ => (None, Some(to_base64(&resp.body)), None, None),
        };

        let text = format!(
            "HTTP {} {} (truncated: {})",
            resp.status,
            resp.url,
            resp.truncated
        );

        let structured = json!({
            "status": resp.status,
            "url": resp.url,
            "headers": resp.headers,
            "contentType": resp.content_type,
            "contentLength": resp.content_length,
            "truncated": resp.truncated,
            "bodyText": body_text,
            "bodyBase64": body_base64,
            "json": json_value,
            "parseError": parse_error,
        });

        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(structured),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "http-tools")]
    #[tool(
        name = "http_request_batch",
        description = "Batch HTTP requests. Each request supports method, headers, cookies, query params, and body. Requires allowlisted domains."
    )]
    async fn http_request_batch(
        &self,
        Parameters(args): Parameters<HttpRequestBatchArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut results: Vec<Option<Value>> = vec![None; args.requests.len()];
        let mut batch_follow = Vec::new();
        let mut batch_no_follow = Vec::new();

        for (index, item) in args.requests.into_iter().enumerate() {
            let _parsed = match self.ensure_http_allowed(&item.request.url) {
                Ok(p) => p,
                Err(e) => {
                    results[index] = Some(json!({
                        "id": item.id,
                        "ok": false,
                        "error": e.to_string(),
                    }));
                    continue;
                }
            };

            let mut body_bytes = None;
            if let Some(path) = &item.request.body_path {
                match self.resolve(path).await {
                    Ok(resolved) => match fs::read(&resolved).await {
                        Ok(bytes) => body_bytes = Some(bytes),
                        Err(e) => {
                            results[index] = Some(json!({
                                "id": item.id,
                                "ok": false,
                                "error": format!("Failed to read body file: {e}"),
                            }));
                            continue;
                        }
                    },
                    Err(e) => {
                        results[index] = Some(json!({
                            "id": item.id,
                            "ok": false,
                            "error": e.to_string(),
                        }));
                        continue;
                    }
                }
            }

            let params = HttpRequestParams {
                method: item.request.method.clone(),
                url: item.request.url.clone(),
                headers: item.request.headers.clone(),
                cookies: item.request.cookies.clone(),
                query: item.request.query.clone(),
                body: item.request.body.clone(),
                body_base64: item.request.body_base64,
                body_bytes,
                timeout_ms: item.request.timeout_ms,
                max_bytes: item.request.max_bytes,
            };

            let request_item = crate::tools::http_tools::HttpRequestItem {
                id: item.id,
                params,
            };

            if item.request.follow_redirects {
                batch_follow.push((index, request_item));
            } else {
                batch_no_follow.push((index, request_item));
            }
        }

        for (follow, items) in [(true, batch_follow), (false, batch_no_follow)] {
            if items.is_empty() {
                continue;
            }

            let client = self.http_client(follow);
            let (indices, batch_items): (Vec<usize>, Vec<crate::tools::http_tools::HttpRequestItem>) =
                items.into_iter().unzip();
            let batch_results = http_request_batch(client, batch_items).await;

            for (index, result) in indices.into_iter().zip(batch_results) {
                let json_result = match result {
                    crate::tools::http_tools::HttpBatchResult { id, ok, response, error } => {
                        if ok {
                            if let Some(resp) = response {
                                json!({
                                    "id": id,
                                    "ok": true,
                                    "status": resp.status,
                                    "url": resp.url,
                                    "headers": resp.headers,
                                    "contentType": resp.content_type,
                                    "contentLength": resp.content_length,
                                    "truncated": resp.truncated,
                                    "bodyBase64": to_base64(&resp.body),
                                })
                            } else {
                                json!({ "id": id, "ok": false, "error": "Missing response" })
                            }
                        } else {
                            json!({ "id": id, "ok": false, "error": error })
                        }
                    }
                };
                results[index] = Some(json_result);
            }
        }

        let results: Vec<Value> = results
            .into_iter()
            .map(|r| r.unwrap_or_else(|| json!({ "ok": false, "error": "Missing result" })))
            .collect();

        Ok(CallToolResult {
            content: vec![Content::text(format!("Batch results: {}", results.len()))],
            structured_content: Some(json!({ "results": results })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "http-tools")]
    #[tool(
        name = "http_download",
        description = "Download an HTTP/HTTPS resource to a local file path. Requires allowlisted domains."
    )]
    async fn http_download(
        &self,
        Parameters(args): Parameters<HttpDownloadArgs>,
    ) -> Result<CallToolResult, McpError> {
        let _parsed = self.ensure_http_allowed(&args.url)?;

        let params = HttpRequestParams {
            method: "GET".to_string(),
            url: args.url.clone(),
            headers: args.headers.clone(),
            cookies: args.cookies.clone(),
            query: args.query.clone(),
            body: None,
            body_base64: false,
            body_bytes: None,
            timeout_ms: args.timeout_ms,
            max_bytes: args.max_bytes,
        };

        let client = self.http_client(args.follow_redirects);
        let resp = http_request(client, params)
            .await
            .map_err(|e| McpError::internal_error(format!("HTTP request failed: {e}"), None))?;

        let path = self.resolve(&args.path).await?;
        fs::write(&path, &resp.body)
            .await
            .map_err(internal_err("Failed to write download"))?;

        let text = format!("Downloaded {} bytes to {}", resp.body.len(), path.display());
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "path": path.to_string_lossy(),
                "bytes": resp.body.len(),
                "status": resp.status,
                "url": resp.url,
                "truncated": resp.truncated,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "http-tools")]
    #[tool(
        name = "http_download_batch",
        description = "Batch HTTP downloads to local file paths. Requires allowlisted domains."
    )]
    async fn http_download_batch(
        &self,
        Parameters(args): Parameters<HttpDownloadBatchArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut results = Vec::new();
        for item in args.downloads {
            let _parsed = match self.ensure_http_allowed(&item.url) {
                Ok(p) => p,
                Err(e) => {
                    results.push(json!({
                        "url": item.url,
                        "path": item.path,
                        "ok": false,
                        "error": e.to_string(),
                    }));
                    continue;
                }
            };

            let params = HttpRequestParams {
                method: "GET".to_string(),
                url: item.url.clone(),
                headers: item.headers.clone(),
                cookies: item.cookies.clone(),
                query: item.query.clone(),
                body: None,
                body_base64: false,
                body_bytes: None,
                timeout_ms: item.timeout_ms,
                max_bytes: item.max_bytes,
            };

            let client = self.http_client(item.follow_redirects);
            match http_request(client, params).await {
                Ok(resp) => {
                    match self.resolve(&item.path).await {
                        Ok(path) => {
                            if let Err(e) = fs::write(&path, &resp.body).await {
                                results.push(json!({
                                    "url": item.url,
                                    "path": item.path,
                                    "ok": false,
                                    "error": format!("Failed to write file: {e}"),
                                }));
                                continue;
                            }
                            results.push(json!({
                                "url": item.url,
                                "path": path.to_string_lossy(),
                                "ok": true,
                                "status": resp.status,
                                "bytes": resp.body.len(),
                                "truncated": resp.truncated,
                            }));
                        }
                        Err(e) => {
                            results.push(json!({
                                "url": item.url,
                                "path": item.path,
                                "ok": false,
                                "error": e.to_string(),
                            }));
                        }
                    }
                }
                Err(e) => {
                    results.push(json!({
                        "url": item.url,
                        "path": item.path,
                        "ok": false,
                        "error": e.to_string(),
                    }));
                }
            }
        }

        Ok(CallToolResult {
            content: vec![Content::text(format!("Batch downloads: {}", results.len()))],
            structured_content: Some(json!({ "results": results })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_list_buckets",
        description = "List S3 buckets for the current credentials. Requires allowlisted buckets (use '*' to allow all)."
    )]
    async fn s3_list_buckets(
        &self,
        Parameters(args): Parameters<S3ListBucketsArgs>,
    ) -> Result<CallToolResult, McpError> {
        if self.s3_allowlist_buckets.is_empty() {
            return Err(McpError::invalid_params(
                "S3 allowlist is empty; use FS_MCP_S3_ALLOW_LIST or --s3-allowlist-bucket",
                None,
            ));
        }
        if !self.s3_allowlist_buckets.iter().any(|b| b == "*") {
            return Err(McpError::invalid_params(
                "S3 allowlist must include '*' to list all buckets",
                None,
            ));
        }

        let client = self.s3_client_for(&args.credentials).await?;
        let buckets = list_buckets(&client)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 list buckets failed: {e}"), None))?;

        let text = format!("S3 buckets: {}", buckets.len());
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "buckets": buckets.iter().map(|b| json!({
                    "name": b.name,
                    "createdAt": b.created_at,
                })).collect::<Vec<_>>(),
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_list",
        description = "List S3 objects and common prefixes. Requires allowlisted buckets."
    )]
    async fn s3_list(
        &self,
        Parameters(args): Parameters<S3ListArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }

        let client = self.s3_client_for(&args.credentials).await?;
        let params = S3ListParams {
            bucket: args.bucket.clone(),
            prefix: args.prefix.clone(),
            delimiter: args.delimiter.clone(),
            max_keys: args.max_keys,
            continuation_token: args.continuation_token.clone(),
        };
        let result = list_objects(&client, params)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 list failed: {e}"), None))?;

        let text = format!(
            "S3 list: {} objects, {} prefixes",
            result.objects.len(),
            result.prefixes.len()
        );
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "objects": result.objects.iter().map(|o| json!({
                    "key": o.key,
                    "size": o.size,
                    "eTag": o.e_tag,
                    "lastModified": o.last_modified,
                    "storageClass": o.storage_class,
                })).collect::<Vec<_>>(),
                "prefixes": result.prefixes,
                "isTruncated": result.is_truncated,
                "nextToken": result.next_token,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_stat",
        description = "Fetch S3 object metadata. Requires allowlisted buckets."
    )]
    async fn s3_stat(
        &self,
        Parameters(args): Parameters<S3StatArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }

        let client = self.s3_client_for(&args.credentials).await?;
        let result = stat_object(&client, &args.bucket, &args.key)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 stat failed: {e}"), None))?;

        let text = format!("S3 stat: {}/{}", result.bucket, result.key);
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "bucket": result.bucket,
                "key": result.key,
                "size": result.size,
                "eTag": result.e_tag,
                "contentType": result.content_type,
                "lastModified": result.last_modified,
                "metadata": result.metadata,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_get",
        description = "Get S3 object bytes or write to file. Requires allowlisted buckets."
    )]
    async fn s3_get(
        &self,
        Parameters(args): Parameters<S3GetArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }

        let output_path = if let Some(path) = &args.output_path {
            Some(self.resolve(path).await?.to_string_lossy().to_string())
        } else {
            None
        };

        let client = self.s3_client_for(&args.credentials).await?;
        let params = S3GetParams {
            bucket: args.bucket.clone(),
            key: args.key.clone(),
            range: args.range.clone(),
            output_path,
            max_bytes: Some(args.max_bytes),
            accept_text: args.accept_text,
        };
        let result = get_object(&client, params)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 get failed: {e}"), None))?;

        let text = if let Some(path) = &result.output_path {
            format!("Downloaded to {}", path)
        } else {
            format!("S3 get: {}/{}", result.bucket, result.key)
        };
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "bucket": result.bucket,
                "key": result.key,
                "size": result.size,
                "contentType": result.content_type,
                "bodyBase64": result.body.as_ref().map(|b| to_base64(b)),
                "text": result.text,
                "outputPath": result.output_path,
                "truncated": result.truncated,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_put",
        description = "Upload data to S3. Supports path or body. Requires allowlisted buckets."
    )]
    async fn s3_put(
        &self,
        Parameters(args): Parameters<S3PutArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }

        let path = if let Some(p) = &args.path {
            Some(self.resolve(p).await?.to_string_lossy().to_string())
        } else {
            None
        };

        let client = self.s3_client_for(&args.credentials).await?;
        let params = S3PutParams {
            bucket: args.bucket.clone(),
            key: args.key.clone(),
            path,
            body: args.body.clone(),
            body_base64: args.body_base64,
            content_type: args.content_type.clone(),
            cache_control: args.cache_control.clone(),
            metadata: args.metadata.clone(),
        };
        put_object(&client, params)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 put failed: {e}"), None))?;

        Ok(CallToolResult::success(vec![Content::text("S3 put ok")]))
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_copy",
        description = "Copy S3 object. Requires allowlisted buckets."
    )]
    async fn s3_copy(
        &self,
        Parameters(args): Parameters<S3CopyArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.source_bucket, &self.s3_allowlist_buckets)
            || !is_bucket_allowed(&args.dest_bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                "S3 bucket is not in allowlist",
                None,
            ));
        }

        let client = self.s3_client_for(&args.credentials).await?;
        let params = S3CopyParams {
            source_bucket: args.source_bucket.clone(),
            source_key: args.source_key.clone(),
            dest_bucket: args.dest_bucket.clone(),
            dest_key: args.dest_key.clone(),
        };
        copy_object(&client, params)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 copy failed: {e}"), None))?;

        Ok(CallToolResult::success(vec![Content::text("S3 copy ok")]))
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_delete",
        description = "Delete S3 object. Requires allowlisted buckets."
    )]
    async fn s3_delete(
        &self,
        Parameters(args): Parameters<S3DeleteArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }

        let client = self.s3_client_for(&args.credentials).await?;
        delete_object(&client, S3DeleteParams { bucket: args.bucket, key: args.key })
            .await
            .map_err(|e| McpError::internal_error(format!("S3 delete failed: {e}"), None))?;

        Ok(CallToolResult::success(vec![Content::text("S3 delete ok")]))
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_delete_batch",
        description = "Delete multiple S3 objects. Requires allowlisted buckets."
    )]
    async fn s3_delete_batch(
        &self,
        Parameters(args): Parameters<S3DeleteBatchArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }
        let client = self.s3_client_for(&args.credentials).await?;
        delete_objects(&client, &args.bucket, args.keys)
            .await
            .map_err(|e| McpError::internal_error(format!("S3 delete batch failed: {e}"), None))?;
        Ok(CallToolResult::success(vec![Content::text("S3 delete batch ok")]))
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_presign",
        description = "Generate a presigned S3 URL for GET or PUT. Requires allowlisted buckets."
    )]
    async fn s3_presign(
        &self,
        Parameters(args): Parameters<S3PresignArgs>,
    ) -> Result<CallToolResult, McpError> {
        if !is_bucket_allowed(&args.bucket, &self.s3_allowlist_buckets) {
            return Err(McpError::invalid_params(
                format!("S3 bucket '{}' is not in allowlist", args.bucket),
                None,
            ));
        }
        let client = self.s3_client_for(&args.credentials).await?;
        let url = presign(&client, S3PresignParams {
            bucket: args.bucket,
            key: args.key,
            method: args.method,
            expires_in_seconds: args.expires_in_seconds,
        })
        .await
        .map_err(|e| McpError::internal_error(format!("S3 presign failed: {e}"), None))?;

        Ok(CallToolResult {
            content: vec![Content::text(url.clone())],
            structured_content: Some(json!({ "url": url })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_get_batch",
        description = "Batch S3 get. Requires allowlisted buckets."
    )]
    async fn s3_get_batch(
        &self,
        Parameters(args): Parameters<S3GetBatchArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut results = Vec::new();

        for req in args.requests {
            if !is_bucket_allowed(&req.bucket, &self.s3_allowlist_buckets) {
                results.push(json!({
                    "bucket": req.bucket,
                    "key": req.key,
                    "ok": false,
                    "error": "Bucket not in allowlist",
                }));
                continue;
            }

            let client = match self.s3_client_for(&req.credentials).await {
                Ok(client) => client,
                Err(e) => {
                    results.push(json!({
                        "bucket": req.bucket,
                        "key": req.key,
                        "ok": false,
                        "error": e.to_string(),
                    }));
                    continue;
                }
            };

            let output_path = if let Some(path) = &req.output_path {
                match self.resolve(path).await {
                    Ok(resolved) => Some(resolved.to_string_lossy().to_string()),
                    Err(e) => {
                        results.push(json!({
                            "bucket": req.bucket,
                            "key": req.key,
                            "ok": false,
                            "error": e.to_string(),
                        }));
                        continue;
                    }
                }
            } else {
                None
            };

            let params = S3GetParams {
                bucket: req.bucket.clone(),
                key: req.key.clone(),
                range: req.range.clone(),
                output_path,
                max_bytes: Some(req.max_bytes),
                accept_text: req.accept_text,
            };

            match get_object(&client, params).await {
                Ok(result) => results.push(json!({
                    "bucket": result.bucket,
                    "key": result.key,
                    "ok": true,
                    "size": result.size,
                    "contentType": result.content_type,
                    "bodyBase64": result.body.as_ref().map(|b| to_base64(b)),
                    "text": result.text,
                    "outputPath": result.output_path,
                    "truncated": result.truncated,
                })),
                Err(e) => results.push(json!({
                    "bucket": req.bucket,
                    "key": req.key,
                    "ok": false,
                    "error": e.to_string(),
                })),
            }
        }

        Ok(CallToolResult {
            content: vec![Content::text(format!("Batch S3 get: {}", results.len()))],
            structured_content: Some(json!({ "results": results })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_put_batch",
        description = "Batch S3 put. Requires allowlisted buckets."
    )]
    async fn s3_put_batch(
        &self,
        Parameters(args): Parameters<S3PutBatchArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut results = Vec::new();

        for req in args.requests {
            if !is_bucket_allowed(&req.bucket, &self.s3_allowlist_buckets) {
                results.push(json!({
                    "bucket": req.bucket,
                    "key": req.key,
                    "ok": false,
                    "error": "Bucket not in allowlist",
                }));
                continue;
            }

            let client = match self.s3_client_for(&req.credentials).await {
                Ok(client) => client,
                Err(e) => {
                    results.push(json!({
                        "bucket": req.bucket,
                        "key": req.key,
                        "ok": false,
                        "error": e.to_string(),
                    }));
                    continue;
                }
            };

            let path = if let Some(p) = &req.path {
                match self.resolve(p).await {
                    Ok(resolved) => Some(resolved.to_string_lossy().to_string()),
                    Err(e) => {
                        results.push(json!({
                            "bucket": req.bucket,
                            "key": req.key,
                            "ok": false,
                            "error": e.to_string(),
                        }));
                        continue;
                    }
                }
            } else {
                None
            };

            let params = S3PutParams {
                bucket: req.bucket.clone(),
                key: req.key.clone(),
                path,
                body: req.body.clone(),
                body_base64: req.body_base64,
                content_type: req.content_type.clone(),
                cache_control: req.cache_control.clone(),
                metadata: req.metadata.clone(),
            };

            match put_object(&client, params).await {
                Ok(()) => results.push(json!({
                    "bucket": req.bucket,
                    "key": req.key,
                    "ok": true,
                })),
                Err(e) => results.push(json!({
                    "bucket": req.bucket,
                    "key": req.key,
                    "ok": false,
                    "error": e.to_string(),
                })),
            }
        }

        Ok(CallToolResult {
            content: vec![Content::text(format!("Batch S3 put: {}", results.len()))],
            structured_content: Some(json!({ "results": results })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[cfg(feature = "s3-tools")]
    #[tool(
        name = "s3_copy_batch",
        description = "Batch S3 copy. Requires allowlisted buckets."
    )]
    async fn s3_copy_batch(
        &self,
        Parameters(args): Parameters<S3CopyBatchArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut results = Vec::new();

        for req in args.requests {
            if !is_bucket_allowed(&req.source_bucket, &self.s3_allowlist_buckets)
                || !is_bucket_allowed(&req.dest_bucket, &self.s3_allowlist_buckets) {
                results.push(json!({
                    "sourceBucket": req.source_bucket,
                    "destBucket": req.dest_bucket,
                    "key": req.source_key,
                    "ok": false,
                    "error": "Bucket not in allowlist",
                }));
                continue;
            }

            let client = match self.s3_client_for(&req.credentials).await {
                Ok(client) => client,
                Err(e) => {
                    results.push(json!({
                        "sourceBucket": req.source_bucket,
                        "destBucket": req.dest_bucket,
                        "key": req.source_key,
                        "ok": false,
                        "error": e.to_string(),
                    }));
                    continue;
                }
            };

            let params = S3CopyParams {
                source_bucket: req.source_bucket.clone(),
                source_key: req.source_key.clone(),
                dest_bucket: req.dest_bucket.clone(),
                dest_key: req.dest_key.clone(),
            };

            match copy_object(&client, params).await {
                Ok(()) => results.push(json!({
                    "sourceBucket": req.source_bucket,
                    "destBucket": req.dest_bucket,
                    "key": req.source_key,
                    "ok": true,
                })),
                Err(e) => results.push(json!({
                    "sourceBucket": req.source_bucket,
                    "destBucket": req.dest_bucket,
                    "key": req.source_key,
                    "ok": false,
                    "error": e.to_string(),
                })),
            }
        }

        Ok(CallToolResult {
            content: vec![Content::text(format!("Batch S3 copy: {}", results.len()))],
            structured_content: Some(json!({ "results": results })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "edit_lines",
        description = "Edit file by LINE NUMBERS (precise, surgical edits). Use when you know EXACT line numbers to modify. Operations: replace (change line(s)), insert_before/insert_after (add new lines), delete (remove line(s)). Supports single lines or ranges (startLine-endLine). Returns unified diff. Use this for: fixing specific lines, adding imports at known positions, removing exact lines. Different from edit_file which uses search/replace text matching. Line numbers are 1-indexed."
    )]
    async fn edit_lines(
        &self,
        Parameters(args): Parameters<EditLinesArgs>,
    ) -> Result<CallToolResult, McpError> {
        // Validate line numbers (1-indexed)
        for (idx, edit) in args.edits.iter().enumerate() {
            if edit.line == 0 {
                return Err(McpError::invalid_params(
                    format!("Edit {}: line number must be >= 1 (1-indexed)", idx),
                    None,
                ));
            }
            if let Some(end) = edit.end_line {
                if end == 0 {
                    return Err(McpError::invalid_params(
                        format!("Edit {}: end_line must be >= 1 (1-indexed)", idx),
                        None,
                    ));
                }
                if end < edit.line {
                    return Err(McpError::invalid_params(
                        format!("Edit {}: invalid range - end_line {} is before line {}", idx, end, edit.line),
                        None,
                    ));
                }
            }
        }

        let path = self.resolve(&args.path).await?;
        let original = read_text(&path)
            .await
            .map_err(internal_err("Failed to read file"))?;

        // Convert JSON operations to internal LineEdit format
        let edits: Vec<LineEdit> = args
            .edits
            .into_iter()
            .map(|e| {
                let operation = match e.operation {
                    LineEditOperation::Replace => LineOperation::Replace,
                    LineEditOperation::InsertBefore => LineOperation::InsertBefore,
                    LineEditOperation::InsertAfter => LineOperation::InsertAfter,
                    LineEditOperation::Delete => LineOperation::Delete,
                };
                LineEdit {
                    start_line: e.line,
                    end_line: e.end_line,
                    operation,
                    text: e.text,
                }
            })
            .collect();

        let (modified, diff) = apply_line_edits(&original, &edits)
            .map_err(|e| McpError::internal_error(format!("Line edit failed: {}", e), None))?;

        if !args.dry_run {
            fs::write(&path, &modified)
                .await
                .map_err(internal_err("Failed to write file"))?;
        }

        let message = if args.dry_run {
            format!("Dry run - changes NOT applied to {}", args.path)
        } else {
            format!("Successfully edited {} lines in {}", edits.len(), args.path)
        };

        Ok(CallToolResult {
            content: vec![Content::text(format!("{}\n\nDiff:\n{}", message, diff))],
            structured_content: Some(json!({
                "message": message,
                "diff": diff,
                "editsApplied": edits.len(),
                "dryRun": args.dry_run,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "bulk_edits",
        description = "Apply SAME edits to MULTIPLE files at once (mass search/replace). Use when you need to change the same code/text across many files. Select files by glob pattern (*.rs, **/*.txt), then apply search/replace operations to all matches. Returns summary of modified files with diffs. Perfect for: renaming functions/variables across codebase, updating imports, fixing typos everywhere, refactoring patterns. More efficient than editing files one by one. Supports dry-run to preview changes.\n\nEach edit supports:\n- isRegex (bool): Use regex pattern instead of literal match. Supports capture groups ($1, $2, etc.)\n- replaceAll (bool): Replace ALL occurrences, not just the first one\n\nBulk behavior:\n- failOnNoMatch (bool): If true, files without matches return errors; if false, they report no changes.\n\nEXAMPLES:\n\n1. Literal replace all occurrences:\n   {\"oldText\": \"use crate::foo\", \"newText\": \"use crate::bar::foo\", \"replaceAll\": true}\n\n2. Regex with capture groups (rename imports):\n   {\"oldText\": \"use crate::(cache_man|event_bus|workers)\", \"newText\": \"use crate::core::$1\", \"isRegex\": true, \"replaceAll\": true}\n\n3. Rename function across codebase:\n   {\"oldText\": \"old_function_name\", \"newText\": \"new_function_name\", \"replaceAll\": true}\n\n4. Update version in all Cargo.toml:\n   {\"oldText\": \"version = \\\"0\\\\.1\\\\.\\\\d+\\\"\", \"newText\": \"version = \\\"0.2.0\\\"\", \"isRegex\": true}"
    )]
    async fn bulk_edits(
        &self,
        Parameters(args): Parameters<BulkEditsArgs>,
    ) -> Result<CallToolResult, McpError> {
        self.ensure_allowed().await?;

        // Convert to FileEdit format
        let edits: Vec<FileEdit> = args
            .edits
            .into_iter()
            .map(|e| FileEdit {
                old_text: e.old_text,
                new_text: e.new_text,
                is_regex: e.is_regex,
                replace_all: e.replace_all,
            })
            .collect();

        let results = bulk_edit_files(
            &args.path,
            &args.file_pattern,
            &args.exclude_patterns,
            &edits,
            args.dry_run,
            args.fail_on_no_match,
            &self.allowed,
            self.allow_symlink_escape,
        )
        .await
        .map_err(|e| McpError::internal_error(format!("Bulk edit failed: {}", e), None))?;

        // Count results
        let total_files = results.len();
        let modified_count = results.iter().filter(|r| r.modified).count();
        let error_count = results.iter().filter(|r| r.error.is_some()).count();

        // Format output
        let mut lines = Vec::new();
        if args.dry_run {
            lines.push("DRY RUN - Changes NOT applied".to_string());
        }
        lines.push(format!(
            "Processed {} files: {} modified, {} errors",
            total_files, modified_count, error_count
        ));
        lines.push(String::new());

        for result in &results {
            let path_str = result.path.to_string_lossy();
            if let Some(err) = &result.error {
                lines.push(format!(" {}: {}", path_str, err));
            } else if result.modified {
                lines.push(format!(" {} - MODIFIED", path_str));
                if let Some(diff) = &result.diff {
                    lines.push(diff.clone());
                    lines.push(String::new());
                }
            } else {
                lines.push(format!("  {} - no changes", path_str));
            }
        }

        let text = lines.join("\n");

        let structured = json!({
            "totalFiles": total_files,
            "modified": modified_count,
            "errors": error_count,
            "dryRun": args.dry_run,
            "results": results.iter().map(|r| json!({
                "path": r.path.to_string_lossy(),
                "modified": r.modified,
                "error": r.error,
                "diff": r.diff,
            })).collect::<Vec<_>>(),
        });

        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(structured),
            is_error: Some(false),
            meta: None,
        })
    }

    // ========================================================================
    // Extract tools - cut content and return it
    // ========================================================================

    #[tool(
        name = "extract_lines",
        description = "Extract (cut) lines from a text file by line numbers. Removes lines from file unless dryRun=true.

PARAMETERS:
- path: File path
- line: Start line (1-indexed, so first line is 1)
- endLine: End line inclusive (optional, defaults to same as 'line' for single line)
- dryRun: If true, only preview - don't modify file
- returnExtracted: If true, include extracted text in response (default: false to save tokens)

EXAMPLES:
- Delete line 5: {path: 'file.txt', line: 5}
- Delete lines 10-20: {path: 'file.txt', line: 10, endLine: 20}
- Preview deletion: {path: 'file.txt', line: 5, dryRun: true}
- Get deleted content: {path: 'file.txt', line: 5, returnExtracted: true}

USE CASES: Remove imports, delete code blocks, cut sections to paste elsewhere."
    )]
    async fn extract_lines(
        &self,
        Parameters(args): Parameters<ExtractLinesArgs>,
    ) -> Result<CallToolResult, McpError> {
        // Validate line numbers (1-indexed)
        if args.line == 0 {
            return Err(McpError::invalid_params(
                "Line number must be >= 1 (1-indexed)",
                None,
            ));
        }
        if let Some(end) = args.end_line {
            if end == 0 {
                return Err(McpError::invalid_params(
                    "End line number must be >= 1 (1-indexed)",
                    None,
                ));
            }
            if end < args.line {
                return Err(McpError::invalid_params(
                    format!("Invalid range: end line {} is before start line {}", end, args.line),
                    None,
                ));
            }
        }

        let path = self.resolve(&args.path).await?;
        let content = read_text(&path)
            .await
            .map_err(internal_err("Failed to read file"))?;

        // Track if original content ends with newline
        let had_trailing_newline = content.ends_with('\n');

        let lines: Vec<&str> = content.lines().collect();
        let start_idx = args.line - 1;
        let end_idx = args.end_line.unwrap_or(args.line) - 1;

        // Validate line numbers
        if start_idx >= lines.len() {
            return Err(McpError::invalid_params(
                format!("Line {} is out of range (file has {} lines)", args.line, lines.len()),
                None,
            ));
        }
        // Clamp end to file length - return what's available
        let end_idx = end_idx.min(lines.len() - 1);
        if start_idx > end_idx {
            return Err(McpError::invalid_params(
                format!("Invalid range: start line {} is after end line {}", args.line, end_idx + 1),
                None,
            ));
        }

        // Extract the lines
        let extracted: Vec<&str> = lines[start_idx..=end_idx].to_vec();
        let extracted_text = extracted.join("\n");
        let line_count = extracted.len();

        if !args.dry_run {
            // Build new content without extracted lines
            let mut remaining: Vec<&str> = Vec::with_capacity(lines.len() - line_count);
            remaining.extend_from_slice(&lines[..start_idx]);
            if end_idx + 1 < lines.len() {
                remaining.extend_from_slice(&lines[end_idx + 1..]);
            }
            let mut new_content = remaining.join("\n");

            // Preserve trailing newline if original had one
            if had_trailing_newline && !new_content.is_empty() {
                new_content.push('\n');
            }

            fs::write(&path, &new_content)
                .await
                .map_err(internal_err("Failed to write file"))?;
        }

        let message = if args.dry_run {
            format!("Dry run - would extract {} line(s) {}-{} from {}", line_count, args.line, end_idx + 1, args.path)
        } else {
            format!("Extracted {} line(s) {}-{} from {}", line_count, args.line, end_idx + 1, args.path)
        };

        // Build response - only include extracted content if requested
        let text_response = if args.return_extracted {
            format!("{}\n\nExtracted content:\n{}", message, extracted_text)
        } else {
            message.clone()
        };

        let mut structured = json!({
            "message": message,
            "lineCount": line_count,
            "startLine": args.line,
            "endLine": end_idx + 1,
            "dryRun": args.dry_run,
        });
        if args.return_extracted {
            structured["extracted"] = json!(extracted_text);
        }

        Ok(CallToolResult {
            content: vec![Content::text(text_response)],
            structured_content: Some(structured),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "extract_symbols",
        description = "Extract (cut) characters from a file by position. Removes chars from file unless dryRun=true.

PARAMETERS:
- path: File path
- start: Start position (0-indexed Unicode chars, not bytes)
- end: End position exclusive (optional) - use EITHER end OR length
- length: Number of chars to extract (optional) - use EITHER end OR length
- dryRun: If true, only preview - don't modify file
- returnExtracted: If true, include extracted text in response (default: false to save tokens)

EXAMPLES:
- First 10 chars: {path: 'file.txt', start: 0, length: 10}
- Chars 100-199: {path: 'file.txt', start: 100, end: 200}
- Preview cut: {path: 'file.txt', start: 50, length: 25, dryRun: true}
- Get cut content: {path: 'file.txt', start: 0, length: 100, returnExtracted: true}

USE CASES: Remove headers, cut text blocks, extract specific character ranges.
Note: Uses Unicode chars (safe for multibyte), not raw bytes. If range exceeds file, returns available content."
    )]
    async fn extract_symbols(
        &self,
        Parameters(args): Parameters<ExtractSymbolsArgs>,
    ) -> Result<CallToolResult, McpError> {
        // Validate args
        if args.end.is_some() && args.length.is_some() {
            return Err(McpError::invalid_params(
                "Specify either 'end' or 'length', not both",
                None,
            ));
        }
        if args.end.is_none() && args.length.is_none() {
            return Err(McpError::invalid_params(
                "Must specify either 'end' or 'length'",
                None,
            ));
        }

        let path = self.resolve(&args.path).await?;
        let content = read_text(&path)
            .await
            .map_err(internal_err("Failed to read file"))?;

        // Work with Unicode characters
        let chars: Vec<char> = content.chars().collect();
        let char_count = chars.len();

        // Clamp start to content length
        let start = args.start.min(char_count);

        // Calculate end position
        let end = if let Some(e) = args.end {
            e.min(char_count)
        } else if let Some(len) = args.length {
            (start + len).min(char_count)
        } else {
            unreachable!()
        };

        if start >= end {
            // Nothing to extract
            return Ok(CallToolResult {
                content: vec![Content::text("Nothing to extract (empty range)")],
                structured_content: Some(json!({
                    "charCount": 0,
                    "start": start,
                    "end": end,
                    "dryRun": args.dry_run,
                })),
                is_error: Some(false),
                meta: None,
            });
        }

        // Extract characters
        let extracted: String = chars[start..end].iter().collect();

        if !args.dry_run {
            // Build new content without extracted chars
            let mut remaining = String::with_capacity(content.len() - extracted.len());
            remaining.extend(chars[..start].iter());
            remaining.extend(chars[end..].iter());

            fs::write(&path, &remaining)
                .await
                .map_err(internal_err("Failed to write file"))?;
        }

        let message = if args.dry_run {
            format!("Dry run - would extract {} characters (positions {}-{}) from {}",
                    end - start, start, end, args.path)
        } else {
            format!("Extracted {} characters (positions {}-{}) from {}",
                    end - start, start, end, args.path)
        };

        // Build response - only include extracted content if requested
        let text_response = if args.return_extracted {
            format!("{}\n\nExtracted content:\n{}", message, extracted)
        } else {
            message.clone()
        };

        let mut structured = json!({
            "message": message,
            "charCount": end - start,
            "start": start,
            "end": end,
            "dryRun": args.dry_run,
        });
        if args.return_extracted {
            structured["extracted"] = json!(extracted);
        }

        Ok(CallToolResult {
            content: vec![Content::text(text_response)],
            structured_content: Some(structured),
            is_error: Some(false),
            meta: None,
        })
    }

    // ========================================================================
    // Binary tools - read/write/edit binary files
    // ========================================================================

    #[tool(
        name = "read_binary",
        description = "Read bytes from a binary file. Returns base64-encoded data.

PARAMETERS:
- path: File path
- offset: Start position in bytes (0-indexed)
- length: Number of bytes to read

EXAMPLES:
- Read 100 bytes from start: {path: 'file.bin', offset: 0, length: 100}
- Read 1KB at position 512: {path: 'file.bin', offset: 512, length: 1024}

USE CASES: Read binary headers, extract sections of images/executables, inspect binary data."
    )]
    async fn read_binary(
        &self,
        Parameters(args): Parameters<ReadBinaryArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;

        let data = read_bytes(&path, args.offset, args.length)
            .await
            .map_err(|e| McpError::internal_error(format!("Failed to read binary: {}", e), None))?;

        let base64_data = to_base64(&data);

        Ok(CallToolResult {
            content: vec![Content::text(format!(
                "Read {} bytes from {} at offset {}\n\nBase64:\n{}",
                data.len(), args.path, args.offset, base64_data
            ))],
            structured_content: Some(json!({
                "data": base64_data,
                "bytesRead": data.len(),
                "offset": args.offset,
                "path": args.path,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "write_binary",
        description = "Write bytes to a binary file. Data must be base64-encoded.

PARAMETERS:
- path: File path
- offset: Position to write at (0-indexed)
- data: Base64-encoded bytes to write
- mode: 'replace' (overwrite) or 'insert' (shift existing bytes)

EXAMPLES:
- Overwrite at pos 0: {path: 'file.bin', offset: 0, data: 'SGVsbG8=', mode: 'replace'}
- Insert at pos 100: {path: 'file.bin', offset: 100, data: 'V29ybGQ=', mode: 'insert'}

USE CASES: Patch executables, inject data into files, modify binary headers. Creates file if missing."
    )]
    async fn write_binary(
        &self,
        Parameters(args): Parameters<WriteBinaryArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;

        let data = from_base64(&args.data)
            .map_err(|e| McpError::invalid_params(format!("Invalid base64: {}", e), None))?;

        let insert = matches!(args.mode, WriteBinaryMode::Insert);

        write_bytes(&path, args.offset, &data, insert)
            .await
            .map_err(|e| McpError::internal_error(format!("Failed to write binary: {}", e), None))?;

        let mode_str = if insert { "inserted" } else { "replaced" };
        let message = format!("Successfully {} {} bytes at offset {} in {}",
                              mode_str, data.len(), args.offset, args.path);

        Ok(CallToolResult {
            content: vec![Content::text(&message)],
            structured_content: Some(json!({
                "message": message,
                "bytesWritten": data.len(),
                "offset": args.offset,
                "mode": if insert { "insert" } else { "replace" },
                "path": args.path,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "extract_binary",
        description = "Extract (cut) bytes from a binary file. Removes bytes and returns base64-encoded data.

PARAMETERS:
- path: File path
- offset: Start position in bytes (0-indexed)
- length: Number of bytes to extract
- dryRun: If true, only preview - don't modify file

EXAMPLES:
- Cut first 256 bytes: {path: 'file.bin', offset: 0, length: 256}
- Preview cut: {path: 'file.bin', offset: 1024, length: 512, dryRun: true}

USE CASES: Remove binary sections, cut data to relocate, strip headers from files."
    )]
    async fn extract_binary(
        &self,
        Parameters(args): Parameters<ExtractBinaryArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;

        let data = if args.dry_run {
            // Just read without removing
            read_bytes(&path, args.offset, args.length)
                .await
                .map_err(|e| McpError::internal_error(format!("Failed to read binary: {}", e), None))?
        } else {
            extract_bytes(&path, args.offset, args.length)
                .await
                .map_err(|e| McpError::internal_error(format!("Failed to extract binary: {}", e), None))?
        };

        let base64_data = to_base64(&data);

        let message = if args.dry_run {
            format!("Dry run - would extract {} bytes at offset {} from {}",
                    data.len(), args.offset, args.path)
        } else {
            format!("Extracted {} bytes at offset {} from {}",
                    data.len(), args.offset, args.path)
        };

        Ok(CallToolResult {
            content: vec![Content::text(format!("{}\n\nBase64:\n{}", message, base64_data))],
            structured_content: Some(json!({
                "message": message,
                "data": base64_data,
                "bytesExtracted": data.len(),
                "offset": args.offset,
                "dryRun": args.dry_run,
                "path": args.path,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "patch_binary",
        description = "Find and replace binary patterns in a file. Both patterns must be base64-encoded.

PARAMETERS:
- path: File path
- find: Base64-encoded pattern to find
- replace: Base64-encoded replacement pattern
- all: If true, replace all occurrences (default: first only)

EXAMPLES:
- Replace first match: {path: 'file.bin', find: 'SGVsbG8=', replace: 'V29ybGQ='}
- Replace all matches: {path: 'file.bin', find: 'AAA=', replace: 'QkJC', all: true}

USE CASES: Patch executables, fix binary data, search-replace in non-text files."
    )]
    async fn patch_binary(
        &self,
        Parameters(args): Parameters<PatchBinaryArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;

        let find_data = from_base64(&args.find)
            .map_err(|e| McpError::invalid_params(format!("Invalid base64 in 'find': {}", e), None))?;
        let replace_data = from_base64(&args.replace)
            .map_err(|e| McpError::invalid_params(format!("Invalid base64 in 'replace': {}", e), None))?;

        let count = patch_bytes(&path, &find_data, &replace_data, args.all)
            .await
            .map_err(|e| McpError::internal_error(format!("Failed to patch binary: {}", e), None))?;

        let message = if count == 0 {
            format!("Pattern not found in {}", args.path)
        } else {
            format!("Replaced {} occurrence(s) in {}", count, args.path)
        };

        Ok(CallToolResult {
            content: vec![Content::text(&message)],
            structured_content: Some(json!({
                "message": message,
                "replacements": count,
                "replaceAll": args.all,
                "path": args.path,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    // ========================================================================
    // NEW TOOLS: Hashing, Comparison, Watch, JSON, PDF, Archives, Stats
    // ========================================================================

    #[tool(
        name = "file_hash",
        description = "Compute hash of a file or file region.\n\nAlgorithms: md5, sha1, sha256 (default), sha512, xxh64, murmur3, spooky.\n\nOptional offset/length for partial hashing (e.g., hash first 1KB: offset=0, length=1024).\n\nEXAMPLES:\n- Hash entire file: {path: 'file.bin'}\n- Hash with MD5: {path: 'file.bin', algorithm: 'md5'}\n- Hash first 1KB: {path: 'file.bin', offset: 0, length: 1024}\n- Hash from position 512: {path: 'file.bin', offset: 512}"
    )]
    async fn file_hash(
        &self,
        Parameters(args): Parameters<FileHashArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        let algo = hash::HashAlgorithm::from_str(args.algorithm.as_deref().unwrap_or("sha256"))
            .map_err(|e| McpError::invalid_params(e.to_string(), None))?;
        
        let result = hash::hash_file_range(&path, algo, args.offset, args.length)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        Ok(CallToolResult {
            content: vec![Content::text(format!("{}: {}", args.path, result.hash))],
            structured_content: Some(json!({
                "path": args.path,
                "algorithm": args.algorithm.as_deref().unwrap_or("sha256"),
                "hash": result.hash,
                "size": result.size,
                "offset": args.offset.unwrap_or(0),
                "length": args.length,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "file_hash_multiple",
        description = "Compute hashes of multiple files.\n\nAlgorithms: md5, sha1, sha256 (default), sha512, xxh64, murmur3, spooky.\n\nReturns all_match=true if all hashes identical. Each result has error field for failures."
    )]
    async fn file_hash_multiple(
        &self,
        Parameters(args): Parameters<FileHashMultipleArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut paths = Vec::new();
        for p in &args.paths {
            paths.push(self.resolve(p).await?);
        }
        
        let algo = hash::HashAlgorithm::from_str(args.algorithm.as_deref().unwrap_or("sha256"))
            .map_err(|e| McpError::invalid_params(e.to_string(), None))?;
        
        let path_refs: Vec<&std::path::Path> = paths.iter().map(|p| p.as_path()).collect();
        let result = hash::hash_files_multiple(&path_refs, algo).await;
        
        let text = result.results.iter()
            .map(|r| format!("{}: {}", r.path, r.hash))
            .collect::<Vec<_>>()
            .join("\n");
        
        Ok(CallToolResult {
            content: vec![Content::text(format!("{}\n\nAll match: {}", text, result.all_match))],
            structured_content: Some(json!({
                "results": result.results.iter().map(|r| json!({
                    "path": r.path,
                    "hash": r.hash,
                    "size": r.size,
                    "error": r.error,
                })).collect::<Vec<_>>(),
                "allMatch": result.all_match,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "compare_files",
        description = "Binary comparison of two files. Returns diff samples (hex bytes), match percentage, hash values, and empty-range flags."
    )]
    async fn compare_files(
        &self,
        Parameters(args): Parameters<CompareFilesArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path1 = self.resolve(&args.path1).await?;
        let path2 = self.resolve(&args.path2).await?;
        
        let params = compare::CompareParams {
            offset1: args.offset1,
            offset2: args.offset2,
            length: if args.length == 0 { None } else { Some(args.length) },
            max_diffs: args.max_diffs,
            context_bytes: args.context_bytes,
        };
        
        let result = compare::compare_files(&path1, &path2, params)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let status = if result.identical { "IDENTICAL" } else { "DIFFERENT" };
        let text = format!(
            "{} vs {}\nStatus: {}\nSize: {} vs {} ({:+})\nMatch: {:.2}%",
            args.path1, args.path2, status,
            result.size1, result.size2, result.size_diff,
            result.match_percentage
        );
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "identical": result.identical,
                "size1": result.size1,
                "size2": result.size2,
                "sizeDiff": result.size_diff,
                "hash1": result.hash1,
                "hash2": result.hash2,
                "firstDiffOffset": result.first_diff_offset,
                "totalDiffRegions": result.total_diff_regions,
                "totalDiffBytes": result.total_diff_bytes,
                "matchPercentage": result.match_percentage,
                "file1Empty": result.file1_empty,
                "file2Empty": result.file2_empty,
                "diffSamples": result.diff_samples.iter().map(|s| json!({
                    "offset": s.offset,
                    "length": s.length,
                    "bytes1Hex": s.bytes1_hex,
                    "bytes2Hex": s.bytes2_hex,
                })).collect::<Vec<_>>(),
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "compare_directories",
        description = "Compare two directories. Returns files only in first, only in second, different files, and any errors encountered."
    )]
    async fn compare_directories(
        &self,
        Parameters(args): Parameters<CompareDirsArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path1 = self.resolve(&args.path1).await?;
        let path2 = self.resolve(&args.path2).await?;
        
        let params = compare::DirCompareParams {
            recursive: args.recursive,
            compare_content: args.compare_content,
            ignore_patterns: args.ignore_patterns.clone(),
        };
        
        let result = compare::compare_directories(&path1, &path2, params)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let status = if result.identical { "IDENTICAL" } else { "DIFFERENT" };
        let text = format!(
            "{}\nOnly in {}: {}\nOnly in {}: {}\nDifferent: {}\nSame: {}",
            status,
            args.path1, result.only_in_first.len(),
            args.path2, result.only_in_second.len(),
            result.different.len(),
            result.same_count
        );
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "identical": result.identical,
                "onlyInFirst": result.only_in_first,
                "onlyInSecond": result.only_in_second,
                "different": result.different.iter().map(|d| json!({
                    "path": d.path,
                    "size1": d.size1,
                    "size2": d.size2,
                    "hash1": d.hash1,
                    "hash2": d.hash2,
                })).collect::<Vec<_>>(),
                "sameCount": result.same_count,
                "diffCount": result.diff_count,
                "errors": result.errors,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "tail_file",
        description = "Read last N lines/bytes of a file. Supports follow mode for logs."
    )]
    async fn tail_file(
        &self,
        Parameters(args): Parameters<TailFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        
        let params = watch::TailParams {
            lines: args.lines,
            bytes: args.bytes.map(|b| b as u64),
            follow: args.follow,
            timeout_ms: args.timeout_ms,
        };
        
        let result = watch::tail_file(&path, params)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        Ok(CallToolResult {
            content: vec![Content::text(&result.content)],
            structured_content: Some(json!({
                "content": result.content,
                "linesReturned": result.lines_returned,
                "fileSize": result.file_size,
                "truncated": result.truncated,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "watch_file",
        description = "Wait for file changes (modify, create, delete). Returns event type, elapsed time, new size, and timedOut flag."
    )]
    async fn watch_file(
        &self,
        Parameters(args): Parameters<WatchFileArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        
        let events: Vec<watch::WatchEvent> = args.events
            .iter()
            .filter_map(|e| watch::WatchEvent::from_str(e).ok())
            .collect();
        
        let result = watch::watch_file(&path, args.timeout_ms, &events)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = if result.changed {
            format!("File changed: {:?} after {}ms", result.event, result.elapsed_ms)
        } else {
            "No changes detected (timeout)".to_string()
        };
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "changed": result.changed,
                "event": result.event,
                "newSize": result.new_size,
                "elapsedMs": result.elapsed_ms,
                "timedOut": result.timed_out,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "read_json",
        description = "Read JSON file with optional JSONPath query. Returns result, totalKeys (objects), arrayLength (arrays), and parse errors."
    )]
    async fn read_json(
        &self,
        Parameters(args): Parameters<ReadJsonArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        
        let result = json_reader::read_json(&path, args.query.as_deref(), args.pretty)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = if let Some(ref err) = result.parse_error {
            format!("JSON parse error at line {:?}, col {:?}: {}\nContext: {:?}",
                err.line, err.column, err.message, err.context)
        } else {
            result.pretty.clone()
        };
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "result": result.result,
                "queryMatched": result.query_matched,
                "parseError": result.parse_error.as_ref().map(|e| json!({
                    "message": e.message,
                    "line": e.line,
                    "column": e.column,
                })),
                "totalKeys": result.total_keys,
                "arrayLength": result.array_length,
            })),
            is_error: Some(result.parse_error.is_some()),
            meta: None,
        })
    }

    #[tool(
        name = "read_pdf",
        description = "Extract text from PDF file. Supports page ranges. Returns text, page count, charCount, and truncation info."
    )]
    async fn read_pdf(
        &self,
        Parameters(args): Parameters<ReadPdfArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        
        let result = pdf_reader::read_pdf(&path, args.pages.as_deref(), args.max_chars)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        Ok(CallToolResult {
            content: vec![Content::text(&result.text)],
            structured_content: Some(json!({
                "text": result.text,
                "pagesCount": result.pages_count,
                "pagesExtracted": result.pages_extracted,
                "truncated": result.truncated,
                "charCount": result.char_count,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "extract_archive",
        description = "Extract archive (zip, tar, tar.gz) to destination."
    )]
    async fn extract_archive(
        &self,
        Parameters(args): Parameters<ExtractArchiveArgs>,
    ) -> Result<CallToolResult, McpError> {
        let archive_path = self.resolve(&args.path).await?;
        let dest_path = self.resolve(&args.destination).await?;
        
        let format = if let Some(ref f) = args.format {
            Some(archive::ArchiveFormat::from_str(f)
                .map_err(|e| McpError::invalid_params(e.to_string(), None))?)
        } else {
            None
        };
        
        let files_filter = if args.files.is_empty() { None } else { Some(args.files.as_slice()) };
        
        let result = archive::extract_archive(&archive_path, &dest_path, format, files_filter)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = format!(
            "Extracted {} files ({} dirs) to {}. Total: {} bytes",
            result.files_extracted, result.dirs_created,
            args.destination, result.total_bytes
        );
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "filesExtracted": result.files_extracted,
                "dirsCreated": result.dirs_created,
                "files": result.files,
                "totalBytes": result.total_bytes,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "create_archive",
        description = "Create archive (zip, tar, tar.gz) from files/directories."
    )]
    async fn create_archive(
        &self,
        Parameters(args): Parameters<CreateArchiveArgs>,
    ) -> Result<CallToolResult, McpError> {
        let mut paths = Vec::new();
        for p in &args.paths {
            paths.push(self.resolve(p).await?);
        }
        let dest_path = self.resolve(&args.destination).await?;
        
        let format = if let Some(ref f) = args.format {
            Some(archive::ArchiveFormat::from_str(f)
                .map_err(|e| McpError::invalid_params(e.to_string(), None))?)
        } else {
            None
        };
        
        let result = archive::create_archive(&paths, &dest_path, format)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = format!(
            "Created archive with {} files. Size: {} bytes",
            result.files_added, result.archive_size
        );
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "filesAdded": result.files_added,
                "archiveSize": result.archive_size,
                "archivePath": result.archive_path,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "file_stats",
        description = "Get statistics for file/directory: total files, size, breakdown by extension, largest files."
    )]
    async fn file_stats(
        &self,
        Parameters(args): Parameters<FileStatsArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        
        let result = stats::file_stats(&path, args.recursive, 10)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = format!(
            "Files: {}, Dirs: {}, Size: {}",
            result.total_files, result.total_dirs, result.total_size_human
        );
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "totalFiles": result.total_files,
                "totalDirs": result.total_dirs,
                "totalSize": result.total_size,
                "totalSizeHuman": result.total_size_human,
                "byExtension": result.by_extension.iter().map(|(k, v)| {
                    (k.clone(), json!({ "count": v.count, "size": v.size }))
                }).collect::<std::collections::HashMap<_, _>>(),
                "largestFiles": result.largest_files.iter().map(|f| json!({
                    "path": f.path,
                    "size": f.size,
                })).collect::<Vec<_>>(),
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "find_duplicates",
        description = "Find duplicate files by content hash. Returns groups of duplicates and wasted space."
    )]
    async fn find_duplicates(
        &self,
        Parameters(args): Parameters<FindDuplicatesArgs>,
    ) -> Result<CallToolResult, McpError> {
        let path = self.resolve(&args.path).await?;
        
        let result = duplicates::find_duplicates(&path, args.min_size, args.by_content)
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = format!(
            "Found {} duplicate groups ({} files). Wasted space: {}",
            result.duplicate_groups.len(),
            result.duplicate_files,
            result.wasted_space_human
        );
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "duplicateGroups": result.duplicate_groups.iter().map(|g| json!({
                    "hash": g.hash,
                    "size": g.size,
                    "files": g.files,
                })).collect::<Vec<_>>(),
                "totalWastedSpace": result.total_wasted_space,
                "wastedSpaceHuman": result.wasted_space_human,
                "filesScanned": result.files_scanned,
                "duplicateFiles": result.duplicate_files,
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    // === Process Management Tools ===

    #[tool(
        name = "run_command",
        description = "Execute a shell command with full control over execution environment.\n\n\
            CROSS-PLATFORM: Works on Windows, macOS, and Linux.\n\n\
            **Features:**\n\
            - Custom working directory (cwd)\n\
            - Environment variables (env) - added to current environment\n\
            - Timeout with automatic kill (timeout_ms)\n\
            - Redirect stdout/stderr to files\n\
            - Read stdin from file\n\
            - Tail output (stdout_tail/stderr_tail) - return only last N lines\n\
            - Background execution (background) - returns PID immediately\n\
            - Background output: use stdout_file/stderr_file to capture output while running\n\n\
            **Examples:**\n\
            - Run Python script: {command: 'python', args: ['script.py']}\n\
            - With timeout: {command: 'cargo', args: ['build'], timeout_ms: 60000}\n\
            - Background: {command: 'npm', args: ['start'], background: true}\n\
            - Custom env: {command: 'node', args: ['app.js'], env: {NODE_ENV: 'production'}}\n\
            - Tail output: {command: 'cargo', args: ['test'], stdout_tail: 50}"
    )]
    async fn run_command(
        &self,
        Parameters(args): Parameters<RunCommandArgs>,
    ) -> Result<CallToolResult, McpError> {
        let args_refs: Vec<&str> = args.args.iter().map(|s| s.as_str()).collect();
        
        let params = process::RunParams {
            cwd: args.cwd,
            env: args.env,
            clear_env: args.clear_env,
            timeout_ms: args.timeout_ms,
            kill_after_ms: args.timeout_ms, // Use same value for watchdog
            stdout_file: args.stdout_file,
            stderr_file: args.stderr_file,
            stdin_file: args.stdin_file,
            stdout_tail: args.stdout_tail,
            stderr_tail: args.stderr_tail,
            background: args.background,
        };
        
        let result = process::run_command(&args.command, &args_refs, params, Some(&self.process_manager))
            .await
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let status = if result.background {
            format!("Started in background (PID: {})", result.pid.unwrap_or(0))
        } else if result.killed {
            format!("Killed after {}ms (timeout)", result.duration_ms)
        } else {
            format!("Completed in {}ms (exit code: {:?})", result.duration_ms, result.exit_code)
        };
        
        let mut text_parts = vec![status];
        if !result.stdout.is_empty() {
            text_parts.push(format!("\n--- stdout ---\n{}", result.stdout));
        }
        if !result.stderr.is_empty() {
            text_parts.push(format!("\n--- stderr ---\n{}", result.stderr));
        }
        
        Ok(CallToolResult {
            content: vec![Content::text(text_parts.join(""))],
            structured_content: Some(json!({
                "exitCode": result.exit_code,
                "stdout": result.stdout,
                "stderr": result.stderr,
                "pid": result.pid,
                "killed": result.killed,
                "timedOut": result.timed_out,
                "durationMs": result.duration_ms,
                "background": result.background,
            })),
            is_error: Some(result.exit_code.map(|c| c != 0).unwrap_or(result.killed)),
            meta: None,
        })
    }

    #[tool(
        name = "kill_process",
        description = "Kill a running process by PID.\n\n\
            CROSS-PLATFORM: Works on Windows (taskkill), macOS and Linux (kill).\n\n\
            **Parameters:**\n\
            - pid: Process ID to kill\n\
            - force: Force kill (SIGKILL on Unix, /F on Windows)\n\n\
            **Examples:**\n\
            - Graceful: {pid: 12345}\n\
            - Force kill: {pid: 12345, force: true}"
    )]
    async fn kill_process(
        &self,
        Parameters(args): Parameters<KillProcessArgs>,
    ) -> Result<CallToolResult, McpError> {
        let success = process::kill_process(args.pid, args.force)
            .map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        // Unregister from our process manager if we were tracking it
        self.process_manager.unregister(args.pid).await;
        
        let text = if success {
            format!("Process {} killed successfully", args.pid)
        } else {
            format!("Failed to kill process {} (may not exist)", args.pid)
        };
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "pid": args.pid,
                "success": success,
            })),
            is_error: Some(!success),
            meta: None,
        })
    }

    #[tool(
        name = "list_processes",
        description = "List background processes started by this server.\n\n\
            Returns processes that were started with run_command(background: true) and are still tracked.\n\
            Note: This only lists processes started by THIS server session, not all system processes.\n\n\
            **Parameters:**\n\
            - filter: Optional filter by command name"
    )]
    async fn list_processes(
        &self,
        Parameters(args): Parameters<ListProcessesArgs>,
    ) -> Result<CallToolResult, McpError> {
        let processes = self.process_manager.list().await;
        
        let filtered: Vec<_> = if let Some(ref filter) = args.filter {
            processes.into_iter()
                .filter(|p| p.command.contains(filter))
                .collect()
        } else {
            processes
        };
        
        let text = if filtered.is_empty() {
            "No background processes tracked".to_string()
        } else {
            filtered.iter()
                .map(|p| format!(
                    "PID {}: {} {} (running for {}s)",
                    p.pid,
                    p.command,
                    p.args.join(" "),
                    p.started_at.elapsed().as_secs()
                ))
                .collect::<Vec<_>>()
                .join("\n")
        };
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "processes": filtered.iter().map(|p| json!({
                    "pid": p.pid,
                    "command": p.command,
                    "args": p.args,
                    "cwd": p.cwd,
                    "runningForSecs": p.started_at.elapsed().as_secs(),
                })).collect::<Vec<_>>(),
                "count": filtered.len(),
            })),
            is_error: Some(false),
            meta: None,
        })
    }

    #[tool(
        name = "search_processes",
        description = "Search for running processes by name or command line pattern.\n\n\
            CROSS-PLATFORM: Works on Windows, macOS, and Linux.\n\n\
            **Parameters:**\n\
            - name_pattern: Regex to match process name (e.g., 'chrome', 'python.*')\n\
            - cmdline_pattern: Regex to match full command line (e.g., '--port=8080')\n\
            - include_window_title: Include window titles (Windows only, slower)\n\n\
            **Examples:**\n\
            - Find Chrome: {name_pattern: 'chrome'}\n\
            - Find by port: {cmdline_pattern: '--port=3000'}\n\
            - Find Python scripts: {name_pattern: 'python', cmdline_pattern: 'script\\\\.py'}\n\n\
            **Note:** At least one of name_pattern or cmdline_pattern must be provided."
    )]
    async fn search_processes(
        &self,
        Parameters(args): Parameters<SearchProcessesArgs>,
    ) -> Result<CallToolResult, McpError> {
        if args.name_pattern.is_none() && args.cmdline_pattern.is_none() {
            return Err(McpError::invalid_params(
                "At least one of name_pattern or cmdline_pattern must be provided",
                None,
            ));
        }
        
        let results = process::search_processes(
            args.name_pattern.as_deref(),
            args.cmdline_pattern.as_deref(),
        ).map_err(|e| McpError::internal_error(e.to_string(), None))?;
        
        let text = if results.is_empty() {
            "No matching processes found".to_string()
        } else {
            results.iter()
                .map(|p| {
                    let cmdline = p.command_line.as_deref().unwrap_or("");
                    format!("PID {}: {} ({:.1}% CPU, {} MB) - {}", 
                        p.pid, p.name, p.cpu_percent, p.memory_bytes / 1024 / 1024, cmdline)
                })
                .collect::<Vec<_>>()
                .join("\n")
        };
        
        Ok(CallToolResult {
            content: vec![Content::text(text)],
            structured_content: Some(json!({
                "processes": results,
                "count": results.len(),
            })),
            is_error: Some(false),
            meta: None,
        })
    }
}

#[tool_handler]
impl ServerHandler for FileSystemServer {
    fn get_info(&self) -> ServerInfo {
        self.server_info()
    }

    // NOTE: We don't override initialize() - let rmcp SDK handle the handshake protocol.
    // The default implementation will use get_info() to build the response.
    // Root fetching happens via on_roots_list_changed notification handler.

    async fn on_roots_list_changed(
        &self,
        context: rmcp::service::NotificationContext<rmcp::RoleServer>,
    ) {
        if let Err(err) = self.refresh_roots(&context.peer).await {
            warn!("Failed to refresh roots on list_changed: {}", err);
        }
    }
}

/// Run server in stdio mode (default)
async fn run_stdio_mode(server: FileSystemServer) -> Result<(), Box<dyn std::error::Error>> {
    let transport = stdio();
    let svc = server.serve(transport).await?;
    svc.waiting().await?;
    Ok(())
}

/// Run server in streamable HTTP mode
async fn run_stream_mode(
    server: FileSystemServer,
    bind: &str,
    port: u16,
) -> Result<(), Box<dyn std::error::Error>> {
    use rmcp::transport::StreamableHttpService;
    use rmcp::transport::streamable_http_server::session::local::LocalSessionManager;

    let addr = format!("{}:{}", bind, port);
    tracing::info!("Starting MCP HTTP server on http://{}/mcp", addr);

    // Create service with session management
    let service = StreamableHttpService::new(
        move || Ok(server.clone()),
        LocalSessionManager::default().into(),
        Default::default(),
    );

    // Build router with MCP endpoint and health check
    let router = axum::Router::new()
        .nest_service("/mcp", service)
        .route("/health", axum::routing::get(|| async { "OK" }));

    let tcp_listener = tokio::net::TcpListener::bind(&addr).await?;

    // Start server with graceful shutdown
    axum::serve(tcp_listener, router)
        .with_graceful_shutdown(async {
            tokio::signal::ctrl_c().await.ok();
        })
        .await?;

    Ok(())
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let args = Args::parse();

    // Determine transport mode
    let mode = if args.stream_mode {
        TransportMode::Stream
    } else {
        TransportMode::Stdio
    };

    // Initialize logging based on mode
    // CRITICAL: stdio mode MUST NOT log to stderr by default!
    // Any stderr output during handshake causes "connection closed" in MCP clients
    init_logging(mode, args.log)?;

    // Create server instance
    let allowed = AllowedDirs::new(args.allowed_dirs);
    let mut server = FileSystemServer::new(allowed);
    server.allow_symlink_escape = args.allow_symlink_escape;
    #[cfg(feature = "http-tools")]
    {
        let mut allowlist = args.http_allowlist_domains;
        allowlist.extend(parse_allowlist_env("FS_MCP_HTTP_ALLOW_LIST"));
        server.http_allowlist_domains = allowlist;
    }
    #[cfg(feature = "s3-tools")]
    {
        let mut allowlist = args.s3_allowlist_buckets;
        allowlist.extend(parse_allowlist_env("FS_MCP_S3_ALLOW_LIST"));
        server.s3_allowlist_buckets = allowlist;
    }

    // Run in selected mode
    match mode {
        TransportMode::Stdio => run_stdio_mode(server).await,
        TransportMode::Stream => run_stream_mode(server, &args.bind, args.port).await,
    }
}

// init_tracing removed - see main() comment about why we can't use stderr logging

fn internal_err<T: ToString>(message: &'static str) -> impl FnOnce(T) -> McpError + Clone {
    move |err| {
        let details = err.to_string();
        McpError::internal_error(
            format!("{}: {}", message, details),
            Some(json!({ "error": details }))
        )
    }
}

fn parse_allowlist_env(var_name: &str) -> Vec<String> {
    let Ok(value) = env::var(var_name) else {
        return Vec::new();
    };
    value
        .split(|c: char| c == ',' || c == ';' || c.is_whitespace())
        .filter(|s| !s.is_empty())
        .map(|s| s.to_string())
        .collect()
}

fn service_error(message: &'static str, error: ServiceError) -> McpError {
    let details = error.to_string();
    McpError::internal_error(
        format!("{}: {}", message, details),
        Some(json!({ "error": details }))
    )
}

fn normalize_tool_schemas(tool_router: &mut ToolRouter<FileSystemServer>) {
    for route in tool_router.map.values_mut() {
        let schema_value = Value::Object((*route.attr.input_schema).clone());
        let schema_value = to_draft07_schema(schema_value);
        if let Value::Object(object) = schema_value {
            route.attr.input_schema = object.into();
        }
    }
}

fn to_draft07_schema(mut schema: Value) -> Value {
    if let Value::Object(ref mut root) = schema {
        root.insert(
            "$schema".to_string(),
            Value::String("http://json-schema.org/draft-07/schema#".to_string()),
        );
    }
    rewrite_schema_refs(&mut schema);
    schema
}

fn rewrite_schema_refs(value: &mut Value) {
    match value {
        Value::Object(map) => {
            if let Some(defs) = map.remove("$defs") {
                let definitions = map.entry("definitions".to_string()).or_insert_with(|| Value::Object(Default::default()));
                if let (Value::Object(target), Value::Object(src)) = (definitions, defs) {
                    for (key, value) in src {
                        target.entry(key).or_insert(value);
                    }
                }
            }
            for (key, value) in map.iter_mut() {
                if key == "$ref" {
                    if let Value::String(reference) = value {
                        if let Some(rest) = reference.strip_prefix("#/$defs/") {
                            *reference = format!("#/definitions/{}", rest);
                        } else if reference == "#/$defs" {
                            *reference = "#/definitions".to_string();
                        }
                    }
                }
                rewrite_schema_refs(value);
            }
        }
        Value::Array(items) => {
            for item in items {
                rewrite_schema_refs(item);
            }
        }
        _ => {}
    }
}

fn parse_root_uri(uri: &str) -> Option<PathBuf> {
    if let Ok(url) = url::Url::parse(uri)
        && url.scheme() == "file"
    {
        return url.to_file_path().ok();
    }
    Some(PathBuf::from(uri))
}

fn format_time(time: Option<SystemTime>) -> Option<String> {
    time.and_then(|t| t.duration_since(SystemTime::UNIX_EPOCH).ok())
        .map(|d| {
            let ts = SystemTime::UNIX_EPOCH + Duration::from_secs(d.as_secs());
            humantime::format_rfc3339(ts).to_string()
        })
}

fn permissions_string(meta: &Metadata) -> String {
    #[cfg(unix)]
    {
        format!("{:o}", meta.permissions().mode())
    }
    #[cfg(not(unix))]
    {
        format!("{:?}", meta.permissions())
    }
}

/// Options for directory tree building
struct TreeOptions {
    max_depth: usize,
    show_size: bool,
    show_hash: bool,
}

fn human_size(bytes: u64) -> String {
    const KB: u64 = 1024;
    const MB: u64 = KB * 1024;
    const GB: u64 = MB * 1024;
    const TB: u64 = GB * 1024;
    
    if bytes >= TB {
        format!("{:.2} TB", bytes as f64 / TB as f64)
    } else if bytes >= GB {
        format!("{:.2} GB", bytes as f64 / GB as f64)
    } else if bytes >= MB {
        format!("{:.2} MB", bytes as f64 / MB as f64)
    } else if bytes >= KB {
        format!("{:.2} KB", bytes as f64 / KB as f64)
    } else {
        format!("{} B", bytes)
    }
}

#[async_recursion]
async fn build_tree(
    root: &Path,
    current: &Path,
    exclude: &globset::GlobSet,
    opts: &TreeOptions,
    depth: usize,
) -> Result<Vec<TreeEntry>, McpError> {
    let mut dir = fs::read_dir(current)
        .await
        .map_err(internal_err("Failed to read directory"))?;
    let mut children = Vec::new();
    while let Some(entry) = dir
        .next_entry()
        .await
        .map_err(internal_err("Failed to iterate directory"))?
    {
        let path = entry.path();
        let rel = path.strip_prefix(root).unwrap_or(&path);
        let rel_str = rel.to_string_lossy();
        if exclude.is_match(rel_str.as_ref()) {
            continue;
        }
        
        let file_type = entry
            .file_type()
            .await
            .map_err(internal_err("stat entry"))?;
        let is_dir = file_type.is_dir();
        
        if is_dir {
            // Check depth limit before recursing
            let kids = if depth < opts.max_depth {
                Some(build_tree(root, &path, exclude, opts, depth + 1).await?)
            } else {
                None // At max depth, don't include children
            };
            
            children.push(TreeEntry {
                name: entry.file_name().to_string_lossy().to_string(),
                kind: "directory".to_string(),
                children: kids,
                size: None,
                size_human: None,
                hash: None,
            });
        } else {
            // Get metadata for size
            let (size, size_human) = if opts.show_size {
                if let Ok(meta) = entry.metadata().await {
                    let s = meta.len();
                    (Some(s), Some(human_size(s)))
                } else {
                    (None, None)
                }
            } else {
                (None, None)
            };
            
            // Get hash if requested
            let file_hash = if opts.show_hash {
                hash::hash_file(&path, hash::HashAlgorithm::Sha256)
                    .await
                    .ok()
                    .map(|r| r.hash)
            } else {
                None
            };
            
            children.push(TreeEntry {
                name: entry.file_name().to_string_lossy().to_string(),
                kind: "file".to_string(),
                children: None,
                size,
                size_human,
                hash: file_hash,
            });
        }
    }
    Ok(children)
}

trait WithStructured {
    fn with_structured(self, value: serde_json::Value) -> Self;
}

impl WithStructured for CallToolResult {
    fn with_structured(mut self, value: serde_json::Value) -> Self {
        self.structured_content = Some(value);
        self
    }
}
